{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ea99a53-9589-4e99-8f99-2cfb15afbd92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5624f831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "662fa74e54dc43dbb0deb5b748bdadae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### EPOCH 1 START ###\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da23a5b9ce944f46820c9340ac6580e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/353 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "Proximity300x300.forward() takes 2 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 72\u001b[39m\n\u001b[32m     50\u001b[39m p_metrics = load_metrics(cfg)\n\u001b[32m     52\u001b[39m trainer = Trainer(\n\u001b[32m     53\u001b[39m     train_loader=loaders[\u001b[33m\"\u001b[39m\u001b[33mtrain_triplet\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     54\u001b[39m     val_loader=loaders[\u001b[33m\"\u001b[39m\u001b[33mtest_triplet\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m     start_epoch=\u001b[32m0\u001b[39m,\n\u001b[32m     71\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/net/backups/exports/dataChivo/jpmunoz/chest_ct_retrieval/training/trainer.py:64\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m### EPOCH \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.current_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m START ###\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# Train stage\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m train_loss, metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[38;5;28mself\u001b[39m.last_train_loss = train_loss\n\u001b[32m     66\u001b[39m \u001b[38;5;28mself\u001b[39m.scheduler.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/net/backups/exports/dataChivo/jpmunoz/chest_ct_retrieval/training/trainer.py:235\u001b[39m, in \u001b[36mTrainer.train_epoch\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    232\u001b[39m         target = target.cuda()\n\u001b[32m    234\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer.zero_grad()\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[38;5;66;03m#if type(outputs) not in (tuple, list):\u001b[39;00m\n\u001b[32m    238\u001b[39m \u001b[38;5;66;03m#    outputs = (outputs,)\u001b[39;00m\n\u001b[32m    240\u001b[39m loss_inputs = outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/net/backups/exports/dataChivo/jpmunoz/venvs/proximity/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/net/backups/exports/dataChivo/jpmunoz/venvs/proximity/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[31mTypeError\u001b[39m: Proximity300x300.forward() takes 2 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "from chest_ct_retrieval.training.environment import configure_environment\n",
    "import torch\n",
    "from chest_ct_retrieval.config.config import load_config\n",
    "from chest_ct_retrieval.eval.metric_loader import load_metrics\n",
    "from chest_ct_retrieval.training.data_setup import load_dataset, create_loaders\n",
    "from chest_ct_retrieval.training.model_setup import initialize_model\n",
    "from chest_ct_retrieval.training.setup import setup_training_run\n",
    "from chest_ct_retrieval.training.trainer import Trainer\n",
    "from chest_ct_retrieval.utils.seed import set_seed\n",
    "\n",
    "\n",
    "\n",
    "cfg = load_config(\"chest_ct_retrieval/config/base.yaml\")\n",
    "\n",
    "set_seed(cfg[\"training\"][\"seed\"])\n",
    "\n",
    "cuda_available = torch.cuda.is_available()\n",
    "\n",
    "run_dirs = setup_training_run(cfg[\"paths\"][\"dr2156\"][\"triplet_runs\"])\n",
    "\n",
    "checkpoints_dir = run_dirs[\"checkpoints\"]\n",
    "tensorboard_dir = run_dirs[\"logs\"]\n",
    "\n",
    "configure_environment(cfg)\n",
    "\n",
    "train_set, test_set, neg_compatibles = load_dataset(\n",
    "    cfg[\"paths\"][\"dr2156\"][\"preprocessed_300\"], \n",
    "    cfg[\"training\"][\"seed\"], \n",
    "    float(cfg[\"dataset\"][\"train_frac\"])\n",
    ")\n",
    "\n",
    "p_model, p_loss_fn, p_optimizer, p_scheduler = initialize_model(\n",
    "    embedding_size=int(cfg[\"model\"][\"embedding_size\"]),\n",
    "    margin=float(cfg[\"loss\"][\"margin\"]),\n",
    "    lr=float(cfg[\"training\"][\"optimizer\"][\"lr\"]),\n",
    "    weight_decay=float(cfg[\"training\"][\"optimizer\"][\"weight_decay\"]),\n",
    "    negative_compatibles_dict=neg_compatibles,\n",
    "    print_interval=int(cfg[\"logging\"][\"log_interval\"]),\n",
    "    cuda=cuda_available\n",
    ")\n",
    "\n",
    "loaders = create_loaders(\n",
    "    train_set,\n",
    "    test_set,\n",
    "    cfg[\"training\"][\"batch\"][\"n_classes\"],\n",
    "    cfg[\"training\"][\"batch\"][\"n_samples\"],\n",
    "    cuda_available\n",
    ")\n",
    "\n",
    "p_metrics = load_metrics(cfg)\n",
    "\n",
    "trainer = Trainer(\n",
    "    train_loader=loaders[\"train_triplet\"],\n",
    "    val_loader=loaders[\"test_triplet\"],\n",
    "    train_eval_loader=loaders[\"train_eval\"],\n",
    "    val_eval_loader=loaders[\"test_eval\"],\n",
    "    train_full_loader=loaders[\"all_triplet_train\"],\n",
    "    val_full_loader=loaders[\"all_triplet_test\"],\n",
    "    model=p_model,\n",
    "    loss_fn=p_loss_fn,\n",
    "    optimizer=p_optimizer,\n",
    "    scheduler=p_scheduler,\n",
    "    n_epochs=cfg[\"training\"][\"n_epochs\"],\n",
    "    cuda=cuda_available,\n",
    "    log_interval=cfg[\"logging\"][\"log_interval\"],\n",
    "    checkpoint_dir=checkpoints_dir,\n",
    "    tensorboard_logs_dir=tensorboard_dir,\n",
    "    train_full_loader_switch=cfg[\"training\"][\"train_full_loader_switch\"],\n",
    "    metrics=p_metrics,\n",
    "    start_epoch=0,\n",
    ")\n",
    "trainer.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a893510-a7a9-4c8a-baa0-4dad5c8e7a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths & URLs\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path(os.getcwd()+'/chest_ct_retrieval').resolve()\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# Enable CUDA stacktrace reporting for debugging\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "os.environ['PYTORCH_NO_CUDA_MEMORY_CACHING'] = \"1\"\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "# Directorio base\n",
    "PATH_BASE = '/dataChivo/batch_01/bigdata_02/datareleases/'\n",
    "USER_HOME_PATH = '/dataChivo/jpmunoz/'\n",
    "PROJECT_BASE_PATH = os.path.join(USER_HOME_PATH, 'chest_ct_retrieval')\n",
    "\n",
    "# Data release 2156\n",
    "DR2156_DATARELEASE_PATH = os.path.join(PATH_BASE, 'DR2156')\n",
    "DR2156_DATARELEASE_CT_PATH = os.path.join(DR2156_DATARELEASE_PATH, 'DR2156_studies')\n",
    "DR2156_DATARELEASE_LABELS_PATH = os.path.join(DR2156_DATARELEASE_PATH, 'DR2156_labels', 'DR2156_labels.csv')\n",
    "\n",
    "# Embeddings ResNet18 DR2156 con dimensiones 300x300x300\n",
    "DR2156_300_RESNET_EMBEDDINGS_PATH = os.path.join(PROJECT_BASE_PATH, 'data', 'DR2156', 'DR2156_300_resnet18_embeddings')\n",
    "DR2156_300_PREPROCESSED_PATH = os.path.join(PROJECT_BASE_PATH, 'data', 'DR2156', 'DR2156_300_preprocessed')\n",
    "\n",
    "# Embeddings ResNet18 DR2156 con dimensiones 512x512x300\n",
    "DR2156_512_RESNET_EMBEDDINGS_PATH = os.path.join(PROJECT_BASE_PATH, 'data', 'DR2156', 'DR2156_512_resnet18_embeddings')\n",
    "DR2156_512_PREPROCESSED_PATH = os.path.join(PROJECT_BASE_PATH, 'data', 'DR2156', 'DR2156_512_preprocessed')\n",
    "\n",
    "# Path de modelos basados en tripletas entrenados en el DR2156\n",
    "DR2156_TRIPLET_RUNS_PATH = os.path.join(PROJECT_BASE_PATH, 'runs')\n",
    "#DR2156_TRIPLET_CHECKPOINTS_PATH = os.path.join(PATH_BASE, 'retrieval_models', 'triplets', 'DR2156_checkpoints')\n",
    "#DR2156_TRIPLET_TENSORBOARD_LOGS_DIR = os.path.join(TRIPLET_MODELS_PATH, 'DR2156_logs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f60062d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets.constants import PROXIMITY_VECTOR_LABELS\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "proximity_vector_labels = PROXIMITY_VECTOR_LABELS.items()\n",
    "#proximity_vector_labels = [[0,0,0], [1,0,0], [0,1,0], [0,0,1], [1,0,1], [1,1,0], [0,1,1], [1,1,1]]\n",
    "#proximity_vector_labels = [[1,0,0], [0,1,0], [0,0,1]]\n",
    "#proximity_classes = ['(s/a)', 'c', 'n', 'q', 'c+n', 'c+q', 'n+q', 'c+n+q']\n",
    "proximity_classes = ['(sin anomalías)', 'Condensación', 'Nódulos', 'Quistes', 'Cond.+Nóds.', 'Cond.+Quis.', 'Nóds.+Quis.', 'Cond.+Nóds.+Quis.']\n",
    "proximity_colors = ['#000000', '#ff0000', '#00ff00', '#0000ff',\n",
    "              '#ff00ff', '#ffff00', '#00ffff', '#ffffff']\n",
    "#proximity_colors = ['#ff0000', '#00ff00', '#0000ff']\n",
    "\n",
    "mnist_classes = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728',\n",
    "              '#9467bd', '#8c564b', '#e377c2', '#7f7f7f',\n",
    "              '#bcbd22', '#17becf']\n",
    "\n",
    "def plot_embeddings(embeddings, targets, xlim=None, ylim=None, zlim=None):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = plt.gca()\n",
    "    ax.set_facecolor('#D1D1D1')\n",
    "\n",
    "    xlim = (embeddings[:, 0].min(), embeddings[:, 0].max())\n",
    "    xradius = (xlim[1] - xlim[0]) / 2\n",
    "    xcenter = xlim[0] + xradius\n",
    "    xlim = (xcenter - 1.1*xradius, xcenter + 1.1*xradius)\n",
    "\n",
    "    ylim = (embeddings[:, 1].min(), embeddings[:, 1].max())\n",
    "    yradius = (ylim[1] - ylim[0]) / 2\n",
    "    ycenter = ylim[0] + yradius\n",
    "    ylim = (ycenter - 1.1*yradius, ycenter + 1.1*yradius)\n",
    "    '''\n",
    "    zlim = (embeddings[:, 2].min(), embeddings[:, 2].max())\n",
    "    zradius = (zlim[1] - zlim[0]) / 2\n",
    "    zcenter = zlim[0] + zradius\n",
    "    zlim = (zcenter - 1.1*zradius, zcenter + 1.1*zradius)\n",
    "    '''\n",
    "    #for i in range(len(proximity_vector_labels)):\n",
    "    for i in set(targets):\n",
    "        inds = np.where(targets==i)[0]\n",
    "        #new_mask = [(t == proximity_vector_labels[i]).all() for t in targets]\n",
    "        new_mask = inds\n",
    "        #plt.scatter(embeddings[inds,0], embeddings[inds,1], alpha=0.5, color=colors[i])\n",
    "        #embeddings_x = (embeddings[new_mask,0] - embeddings[new_mask,0].min()) / (embeddings[new_mask,0].max() - embeddings[new_mask,0].min())\n",
    "        #embeddings_y = (embeddings[new_mask,1] - embeddings[new_mask,1].min()) / (embeddings[new_mask,1].max() - embeddings[new_mask,1].min())\n",
    "        #plt.scatter(embeddings_x, embeddings_y, alpha=0.5, edgecolors=proximity_colors[i], color=proximity_colors[i])\n",
    "        ax.scatter(embeddings[new_mask,0], embeddings[new_mask,1], alpha=0.6, edgecolors=proximity_colors[i], color=proximity_colors[i])\n",
    "    \n",
    "    if xlim:\n",
    "        plt.xlim(xlim[0], xlim[1])\n",
    "    if ylim:\n",
    "        plt.ylim(ylim[0], ylim[1])\n",
    "    '''\n",
    "    if zlim:\n",
    "        plt.zlim(zlim[0], zlim[1])\n",
    "    '''\n",
    "    plt.legend(proximity_classes)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9cbd97",
   "metadata": {},
   "source": [
    "# Experimento 3: Entrenamiento por tripletas sobre dataset de volúmenes de tres clases\n",
    "\n",
    "Objetivo: detección de esferas, cubos y tetrahedros en el volumen de entrada---Las clases no se excluyen, o sea, el dataset es multietiqueta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a907d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.config import load_config\n",
    "from utils.seed import set_seed\n",
    "from training.data_setup import load_dataset, create_loaders\n",
    "from training.model_setup import initialize_model\n",
    "\n",
    "cfg = load_config(\"../config/base.yaml\")\n",
    "set_seed(cfg[\"seed\"])\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "train_set, test_set, neg_compatibles = load_dataset(cfg[\"volume_dir\"], cfg[\"seed\"], cfg[\"train_fraction\"])\n",
    "loaders = create_loaders(train_set, test_set, cfg[\"n_classes\"], cfg[\"n_samples\"], cuda)\n",
    "\n",
    "model, loss_fn, optimizer, scheduler = initialize_model(\n",
    "    embedding_size=cfg[\"embedding_size\"],\n",
    "    margin=cfg[\"margin\"],\n",
    "    lr=cfg[\"learning_rate\"],\n",
    "    weight_decay=cfg[\"weight_decay\"],\n",
    "    negative_compatibles_dict=neg_compatibles,\n",
    "    print_interval=10,\n",
    "    cuda=cuda\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d391541",
   "metadata": {},
   "outputs": [],
   "source": [
    "from training.main import main\n",
    "from utils.config import load_config  # as defined earlier\n",
    "\n",
    "cfg = load_config(\"configs/base.yaml\")\n",
    "main(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c61ad0-d7a1-4bea-9ad7-b419b41deb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torchvision.transforms import v2\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import default_collate\n",
    "from utils.compatbility import determine_negative_compatibles\n",
    "from utils.seed import set_seed\n",
    "from datasets.constants import PROXIMITY_VECTOR_LABELS_FOR_TRAINING, PROXIMITY_CLASS_NAMES, PROXIMITY_CLASS_NAMES\n",
    "# Set up the network and training parameters\n",
    "from losses.losses import OnlineTripletLoss\n",
    "from utils.selectors.triplet_selector import HardestNegativeTripletSelector, SemihardNegativeTripletSelector\n",
    "from eval.metrics import AverageNonzeroTripletsMetric, TotalNonzeroTripletsMetric, Loss, NDCG, Recall, AllMetrics\n",
    "from datasets.ct_volume_dataset import ProximityCTEmbeddingTripletDataset\n",
    "from datasets.loaders import *\n",
    "from datasets.samplers import *\n",
    "from models.networks import Proximity300x300\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "seed=0\n",
    "set_seed(seed)\n",
    "\n",
    "volumes_path_list = sorted(Path(DR2156_300_PREPROCESSED_PATH).glob('*.npz'))\n",
    "\n",
    "# generate a list of corresponding labels\n",
    "def get_class_id(label_vector):\n",
    "    for (k, v) in PROXIMITY_VECTOR_LABELS_FOR_TRAINING.items():\n",
    "        equals = label_vector == v\n",
    "        if hasattr(equals, 'all'):\n",
    "            if equals.all():\n",
    "                return k\n",
    "        else:\n",
    "            if equals:\n",
    "                return k\n",
    "    return None\n",
    "\n",
    "samples_path_list = [[]]*len(volumes_path_list)\n",
    "labels_list = [[]]*len(volumes_path_list)\n",
    "for i, p in enumerate(volumes_path_list):\n",
    "    _, fname = os.path.split(p)\n",
    "    info = fname.split('.')[0:-1]\n",
    "    info = ''.join(info)\n",
    "    info = info.split('_')\n",
    "    fid = int(info[0])\n",
    "    vol_id = info[1]\n",
    "    sin_anomalias = int(info[2])\n",
    "    condensacion = int(info[3])\n",
    "    nodulos = int(info[4])\n",
    "    quistes = int(info[5])\n",
    "    samples_path_list[ fid - 1 ] = p\n",
    "    labels_list[ fid - 1 ] = [sin_anomalias, condensacion, nodulos, quistes]\n",
    "labels_as_classes = torch.LongTensor([get_class_id(l) for l in labels_list])\n",
    "\n",
    "negative_compatibles_dict = determine_negative_compatibles(PROXIMITY_VECTOR_LABELS_FOR_TRAINING)\n",
    "\n",
    "train_frac = 0.8\n",
    "x_train, x_test, y_train, y_test = train_test_split(samples_path_list, labels_as_classes, train_size=train_frac, stratify=labels_as_classes, random_state=seed)\n",
    "\n",
    "triplet_training_set = ProximityCTEmbeddingTripletDataset(x_train, y_train, preprocessed=True)\n",
    "triplet_validation_set = ProximityCTEmbeddingTripletDataset(x_test, y_test, preprocessed=True)\n",
    "\n",
    "print('Training set: count for each label')\n",
    "for label, count in enumerate(np.bincount(sorted(triplet_training_set.labels_list))):\n",
    "    print(f'label {label}: {count}')\n",
    "print('Training set size:', len(triplet_training_set))\n",
    "print()\n",
    "print('Validation set: count for each label')\n",
    "for label, count in enumerate(np.bincount(sorted(triplet_validation_set.labels_list))):\n",
    "    print(f'label {label}: {count}')\n",
    "print('Validation set size:', len(triplet_validation_set))\n",
    "\n",
    "def collate(batch):\n",
    "    batch = list(filter(lambda x:x is not None, batch))\n",
    "    return default_collate(batch)\n",
    "\n",
    "# balanced batch sampler: batch size is n_classes*n_samples. Each batch contains n_samples samples for n_classes different classes.\n",
    "sampler_n_classes = 2\n",
    "sampler_n_samples = 4\n",
    "sampler_batch_size = sampler_n_classes * sampler_n_samples\n",
    "train_batch_sampler = ProximityCTEmbeddingTripletDataset(triplet_training_set.labels_list, PROXIMITY_VECTOR_LABELS_FOR_TRAINING, n_classes=sampler_n_classes, n_samples=sampler_n_samples, multilabel=True)\n",
    "test_batch_sampler = ProximityCTEmbeddingTripletDataset(triplet_validation_set.labels_list, PROXIMITY_VECTOR_LABELS_FOR_TRAINING, n_classes=sampler_n_classes, n_samples=sampler_n_samples, multilabel=True)\n",
    "\n",
    "# Set up data loaders\n",
    "#n_classes = 2\n",
    "#batch_size = 8\n",
    "kwargs = {'num_workers': 0, 'pin_memory': True} if cuda else {}\n",
    "train_eval_loader = torch.utils.data.DataLoader(triplet_training_set, batch_size=sampler_batch_size, shuffle=False, **kwargs)\n",
    "test_eval_loader = torch.utils.data.DataLoader(triplet_validation_set, batch_size=sampler_batch_size, shuffle=False, **kwargs)\n",
    "triplet_train_loader = torch.utils.data.DataLoader(triplet_training_set, batch_sampler=train_batch_sampler, **kwargs)\n",
    "triplet_test_loader = torch.utils.data.DataLoader(triplet_validation_set, batch_sampler=test_batch_sampler, **kwargs)\n",
    "all_triplet_train_loader = TripletDataLoader(triplet_training_set, n_classes=sampler_n_classes, n_samples=sampler_n_samples, **kwargs)\n",
    "all_triplet_test_loader = TripletDataLoader(triplet_validation_set, n_classes=sampler_n_classes, n_samples=sampler_n_samples, **kwargs)\n",
    "\n",
    "\n",
    "margin = 0.2\n",
    "embedding_size = 128\n",
    "embedding_net = Proximity300x300(embedding_size=embedding_size)\n",
    "model = embedding_net\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "    \n",
    "loss_fn = OnlineTripletLoss(margin, SemihardNegativeTripletSelector(margin), negative_compatibles_dict, print_interval=10)\n",
    "lr = 1e-5\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, 50, gamma=0.1, last_epoch=-1)\n",
    "n_epochs = 50\n",
    "log_interval = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280458b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = [b for b in train_batch_sampler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d2c60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57aeece8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cbd9f9-e6ba-40d9-b1d5-5ae25358f65c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from training.trainer import Trainer\n",
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now()\n",
    "#timestamp_str = timestamp.strftime('%Y%m%d%H%M%S') \n",
    "timestamp_str_2 = timestamp.strftime('%Y%m%d_%H%M') \n",
    "\n",
    "#log_subdir = f\"slice_size=100x100 train_frac={train_frac} sampler={sampler_n_classes}x{sampler_n_samples} margin={margin} embedding_size={embedding_size} lr={lr} distance=euclidean_squared embedding_l2_norm=1 negative_mining=semihard batch_sampling=oversample_underrepresented_classes timestamp={timestamp_str}\"\n",
    "run_subdir_str = f\"run_{timestamp_str_2}\"\n",
    "run_subdir = os.path.join(cfg[\"paths\"][\"runs\"], run_subdir_str)\n",
    "run_checkpoints_subdir = os.path.join(run_subdir, 'checkpoints')\n",
    "run_logs_subdir = os.path.join(run_subdir, 'logs')\n",
    "\n",
    "os.makedirs(run_subdir, exist_ok=True)\n",
    "os.makedirs(run_checkpoints_subdir, exist_ok=True)\n",
    "os.makedirs(run_logs_subdir, exist_ok=True)\n",
    "\n",
    "start_epoch = 0\n",
    "train_full_loader_switch = False\n",
    "metrics=[\n",
    "    AverageNonzeroTripletsMetric(), \n",
    "    TotalNonzeroTripletsMetric(), \n",
    "    Loss(),\n",
    "    #NDCG(proximity_vector_labels_dict, proximity_class_names),\n",
    "    #Recall(proximity_vector_labels_dict, proximity_class_names),\n",
    "    AllMetrics(PROXIMITY_VECTOR_LABELS_FOR_TRAINING, PROXIMITY_CLASS_NAMES),\n",
    "]\n",
    "\n",
    "trainer = Trainer(\n",
    "    triplet_train_loader, \n",
    "    triplet_test_loader,\n",
    "    train_eval_loader, \n",
    "    test_eval_loader, \n",
    "    all_triplet_train_loader,\n",
    "    all_triplet_test_loader,\n",
    "    model, \n",
    "    loss_fn, \n",
    "    optimizer, \n",
    "    scheduler, \n",
    "    n_epochs, \n",
    "    cuda, \n",
    "    log_interval,\n",
    "    run_checkpoints_subdir,\n",
    "    run_logs_subdir,\n",
    "    train_full_loader_switch,\n",
    "    metrics,\n",
    "    start_epoch,\n",
    ")\n",
    "\n",
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1136318d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aab9aec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4746a31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a2840e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2fcbff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af7d51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(\n",
    "    model.state_dict(),\n",
    "    os.path.join(TRIPLET_CHECKPOINTS_PATH, f'triplets_epoch={32}_val-loss={0.3881}_avg-nonzero-triplets={348588.0}.pth'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f32fa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import extract_embeddings\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "#BEST_MODEL_PATH = os.path.join(TRIPLET_CHECKPOINTS_PATH, 'triplets_val-loss_0.0467_20240424_050251.pth')\n",
    "\n",
    "#best_model = SynthDataEmbeddingModel(embedding_size=3).cuda()\n",
    "#best_model.load_state_dict(torch.load(BEST_MODEL_PATH))\n",
    "\n",
    "train_embeddings_tl, train_labels_tl = extract_embeddings(train_eval_loader, model)\n",
    "train_tsne = TSNE(n_components=2, perplexity=10, learning_rate=\"auto\", init=\"random\")\n",
    "train_embeddings_tsne = train_tsne.fit_transform(train_embeddings_tl.cpu().numpy())\n",
    "plot_embeddings(train_embeddings_tsne, train_labels_tl.numpy())\n",
    "\n",
    "test_embeddings_tl, test_labels_tl = extract_embeddings(test_eval_loader, model)\n",
    "val_tsne = TSNE(n_components=2, perplexity=5, learning_rate=\"auto\", init=\"random\")\n",
    "test_embeddings_tsne = val_tsne.fit_transform(test_embeddings_tl.cpu().numpy())\n",
    "plot_embeddings(test_embeddings_tsne, test_labels_tl.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb72b6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import extract_embeddings\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#BEST_MODEL_PATH = os.path.join(TRIPLET_CHECKPOINTS_PATH, 'triplets_val-loss_0.0467_20240424_050251.pth')\n",
    "\n",
    "#best_model = SynthDataEmbeddingModel(embedding_size=3).cuda()\n",
    "#best_model.load_state_dict(torch.load(BEST_MODEL_PATH))\n",
    "\n",
    "#train_embeddings_tl, train_labels_tl = extract_embeddings(train_eval_loader, model)\n",
    "train_pca = PCA(n_components=2)\n",
    "train_embeddings_pca = train_pca.fit_transform(train_embeddings_tl.cpu().numpy())\n",
    "plot_embeddings(train_embeddings_pca, train_labels_tl.numpy())\n",
    "\n",
    "#test_embeddings_tl, test_labels_tl = extract_embeddings(test_eval_loader, model)\n",
    "val_pca = PCA(n_components=2)\n",
    "test_embeddings_pca = val_pca.fit_transform(test_embeddings_tl.cpu().numpy())\n",
    "plot_embeddings(test_embeddings_pca, test_labels_tl.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ef0202",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import extract_embeddings\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "BEST_MODEL_PATH = os.path.join(TRIPLET_CHECKPOINTS_PATH, 'triplets_epoch=200_val-loss=38.9467_avg-nonzero-triplets=162.0.pth')\n",
    "\n",
    "best_model = SynthDataEmbeddingModel(embedding_size=8).cuda()\n",
    "best_model.load_state_dict(torch.load(BEST_MODEL_PATH))\n",
    "\n",
    "train_embeddings_tl, train_labels_tl = extract_embeddings(train_eval_loader, best_model)\n",
    "train_tsne = TSNE(n_components=2, perplexity=10, learning_rate=\"auto\", init=\"random\")\n",
    "train_embeddings_tsne = train_tsne.fit_transform(train_embeddings_tl.cpu().numpy())\n",
    "plot_embeddings(train_embeddings_tsne, train_labels_tl.numpy())\n",
    "\n",
    "test_embeddings_tl, test_labels_tl = extract_embeddings(test_eval_loader, best_model)\n",
    "val_tsne = TSNE(n_components=2, perplexity=5, learning_rate=\"auto\", init=\"random\")\n",
    "test_embeddings_tsne = val_tsne.fit_transform(test_embeddings_tl.cpu().numpy())\n",
    "plot_embeddings(test_embeddings_tsne, test_labels_tl.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297dd605",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import extract_embeddings\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#train_embeddings_tl, train_labels_tl = extract_embeddings(train_eval_loader, model)\n",
    "train_pca = PCA(n_components=2)\n",
    "train_embeddings_pca = train_pca.fit_transform(train_embeddings_tl.cpu().numpy())\n",
    "plot_embeddings(train_embeddings_pca, train_labels_tl.numpy())\n",
    "\n",
    "#test_embeddings_tl, test_labels_tl = extract_embeddings(test_eval_loader, model)\n",
    "val_pca = PCA(n_components=2)\n",
    "test_embeddings_pca = val_pca.fit_transform(test_embeddings_tl.cpu().numpy())\n",
    "plot_embeddings(test_embeddings_pca, test_labels_tl.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
