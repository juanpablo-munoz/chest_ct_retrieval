{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ea99a53-9589-4e99-8f99-2cfb15afbd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a893510-a7a9-4c8a-baa0-4dad5c8e7a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths & URLs\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path(os.getcwd()+'/chest_ct_retrieval').resolve()\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# Enable CUDA stacktrace reporting for debugging\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "os.environ['PYTORCH_NO_CUDA_MEMORY_CACHING'] = \"1\"\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "# Directorio base\n",
    "PATH_BASE = '/dataChivo/batch_01/bigdata_02/datareleases/'\n",
    "USER_HOME_PATH = '/dataChivo/jpmunoz/'\n",
    "PROJECT_BASE_PATH = os.path.join(USER_HOME_PATH, 'chest_ct_retrieval')\n",
    "\n",
    "# Data release 2156\n",
    "DR2156_DATARELEASE_PATH = os.path.join(PATH_BASE, 'DR2156')\n",
    "DR2156_DATARELEASE_CT_PATH = os.path.join(DR2156_DATARELEASE_PATH, 'DR2156_studies')\n",
    "DR2156_DATARELEASE_LABELS_PATH = os.path.join(DR2156_DATARELEASE_PATH, 'DR2156_labels', 'DR2156_labels.csv')\n",
    "\n",
    "# Embeddings ResNet18 DR2156 con dimensiones 300x300x300\n",
    "DR2156_300_RESNET_EMBEDDINGS_PATH = os.path.join(PROJECT_BASE_PATH, 'data', 'DR2156', 'DR2156_300_resnet18_embeddings')\n",
    "DR2156_300_PREPROCESSED_PATH = os.path.join(PROJECT_BASE_PATH, 'data', 'DR2156', 'DR2156_300_preprocessed')\n",
    "\n",
    "# Embeddings ResNet18 DR2156 con dimensiones 512x512x300\n",
    "DR2156_512_RESNET_EMBEDDINGS_PATH = os.path.join(PROJECT_BASE_PATH, 'data', 'DR2156', 'DR2156_512_resnet18_embeddings')\n",
    "DR2156_512_PREPROCESSED_PATH = os.path.join(PROJECT_BASE_PATH, 'data', 'DR2156', 'DR2156_512_preprocessed')\n",
    "\n",
    "# Path de modelos basados en tripletas entrenados en el DR2156\n",
    "DR2156_TRIPLET_RUNS_PATH = os.path.join(PROJECT_BASE_PATH, 'runs')\n",
    "#DR2156_TRIPLET_CHECKPOINTS_PATH = os.path.join(PATH_BASE, 'retrieval_models', 'triplets', 'DR2156_checkpoints')\n",
    "#DR2156_TRIPLET_TENSORBOARD_LOGS_DIR = os.path.join(TRIPLET_MODELS_PATH, 'DR2156_logs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f60062d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets.constants import PROXIMITY_VECTOR_LABELS\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "proximity_vector_labels = PROXIMITY_VECTOR_LABELS.items()\n",
    "#proximity_vector_labels = [[0,0,0], [1,0,0], [0,1,0], [0,0,1], [1,0,1], [1,1,0], [0,1,1], [1,1,1]]\n",
    "#proximity_vector_labels = [[1,0,0], [0,1,0], [0,0,1]]\n",
    "#proximity_classes = ['(s/a)', 'c', 'n', 'q', 'c+n', 'c+q', 'n+q', 'c+n+q']\n",
    "proximity_classes = ['(sin anomalías)', 'Condensación', 'Nódulos', 'Quistes', 'Cond.+Nóds.', 'Cond.+Quis.', 'Nóds.+Quis.', 'Cond.+Nóds.+Quis.']\n",
    "proximity_colors = ['#000000', '#ff0000', '#00ff00', '#0000ff',\n",
    "              '#ff00ff', '#ffff00', '#00ffff', '#ffffff']\n",
    "#proximity_colors = ['#ff0000', '#00ff00', '#0000ff']\n",
    "\n",
    "mnist_classes = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728',\n",
    "              '#9467bd', '#8c564b', '#e377c2', '#7f7f7f',\n",
    "              '#bcbd22', '#17becf']\n",
    "\n",
    "def plot_embeddings(embeddings, targets, xlim=None, ylim=None, zlim=None):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = plt.gca()\n",
    "    ax.set_facecolor('#D1D1D1')\n",
    "\n",
    "    xlim = (embeddings[:, 0].min(), embeddings[:, 0].max())\n",
    "    xradius = (xlim[1] - xlim[0]) / 2\n",
    "    xcenter = xlim[0] + xradius\n",
    "    xlim = (xcenter - 1.1*xradius, xcenter + 1.1*xradius)\n",
    "\n",
    "    ylim = (embeddings[:, 1].min(), embeddings[:, 1].max())\n",
    "    yradius = (ylim[1] - ylim[0]) / 2\n",
    "    ycenter = ylim[0] + yradius\n",
    "    ylim = (ycenter - 1.1*yradius, ycenter + 1.1*yradius)\n",
    "    '''\n",
    "    zlim = (embeddings[:, 2].min(), embeddings[:, 2].max())\n",
    "    zradius = (zlim[1] - zlim[0]) / 2\n",
    "    zcenter = zlim[0] + zradius\n",
    "    zlim = (zcenter - 1.1*zradius, zcenter + 1.1*zradius)\n",
    "    '''\n",
    "    #for i in range(len(proximity_vector_labels)):\n",
    "    for i in set(targets):\n",
    "        inds = np.where(targets==i)[0]\n",
    "        #new_mask = [(t == proximity_vector_labels[i]).all() for t in targets]\n",
    "        new_mask = inds\n",
    "        #plt.scatter(embeddings[inds,0], embeddings[inds,1], alpha=0.5, color=colors[i])\n",
    "        #embeddings_x = (embeddings[new_mask,0] - embeddings[new_mask,0].min()) / (embeddings[new_mask,0].max() - embeddings[new_mask,0].min())\n",
    "        #embeddings_y = (embeddings[new_mask,1] - embeddings[new_mask,1].min()) / (embeddings[new_mask,1].max() - embeddings[new_mask,1].min())\n",
    "        #plt.scatter(embeddings_x, embeddings_y, alpha=0.5, edgecolors=proximity_colors[i], color=proximity_colors[i])\n",
    "        ax.scatter(embeddings[new_mask,0], embeddings[new_mask,1], alpha=0.6, edgecolors=proximity_colors[i], color=proximity_colors[i])\n",
    "    \n",
    "    if xlim:\n",
    "        plt.xlim(xlim[0], xlim[1])\n",
    "    if ylim:\n",
    "        plt.ylim(ylim[0], ylim[1])\n",
    "    '''\n",
    "    if zlim:\n",
    "        plt.zlim(zlim[0], zlim[1])\n",
    "    '''\n",
    "    plt.legend(proximity_classes)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9cbd97",
   "metadata": {},
   "source": [
    "# Experimento 3: Entrenamiento por tripletas sobre dataset de volúmenes de tres clases\n",
    "\n",
    "Objetivo: detección de esferas, cubos y tetrahedros en el volumen de entrada---Las clases no se excluyen, o sea, el dataset es multietiqueta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a907d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.config import load_config\n",
    "from utils.seed import set_seed\n",
    "from training.data_setup import load_dataset, create_loaders\n",
    "from training.model_setup import initialize_model\n",
    "\n",
    "cfg = load_config(\"../config/base.yaml\")\n",
    "set_seed(cfg[\"seed\"])\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "train_set, test_set, neg_compatibles = load_dataset(cfg[\"volume_dir\"], cfg[\"seed\"], cfg[\"train_fraction\"])\n",
    "loaders = create_loaders(train_set, test_set, cfg[\"n_classes\"], cfg[\"n_samples\"], cuda)\n",
    "\n",
    "model, loss_fn, optimizer, scheduler = initialize_model(\n",
    "    embedding_size=cfg[\"embedding_size\"],\n",
    "    margin=cfg[\"margin\"],\n",
    "    lr=cfg[\"learning_rate\"],\n",
    "    weight_decay=cfg[\"weight_decay\"],\n",
    "    negative_compatibles_dict=neg_compatibles,\n",
    "    print_interval=10,\n",
    "    cuda=cuda\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c61ad0-d7a1-4bea-9ad7-b419b41deb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torchvision.transforms import v2\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import default_collate\n",
    "from utils.compatbility import determine_negative_compatibles\n",
    "from utils.seed import set_seed\n",
    "from datasets.constants import PROXIMITY_VECTOR_LABELS_FOR_TRAINING, PROXIMITY_CLASS_NAMES, PROXIMITY_CLASS_NAMES\n",
    "# Set up the network and training parameters\n",
    "from losses.losses import OnlineTripletLoss\n",
    "from utils.selectors.triplet_selector import HardestNegativeTripletSelector, SemihardNegativeTripletSelector\n",
    "from eval.metrics import AverageNonzeroTripletsMetric, TotalNonzeroTripletsMetric, Loss, NDCG, Recall, AllMetrics\n",
    "from datasets.ct_volume_dataset import ProximityCTEmbeddingTripletDataset\n",
    "from datasets.loaders import *\n",
    "from datasets.samplers import *\n",
    "from models.networks import Proximity300x300\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "seed=0\n",
    "set_seed(seed)\n",
    "\n",
    "volumes_path_list = sorted(Path(DR2156_300_PREPROCESSED_PATH).glob('*.npz'))\n",
    "\n",
    "# generate a list of corresponding labels\n",
    "def get_class_id(label_vector):\n",
    "    for (k, v) in PROXIMITY_VECTOR_LABELS_FOR_TRAINING.items():\n",
    "        equals = label_vector == v\n",
    "        if hasattr(equals, 'all'):\n",
    "            if equals.all():\n",
    "                return k\n",
    "        else:\n",
    "            if equals:\n",
    "                return k\n",
    "    return None\n",
    "\n",
    "samples_path_list = [[]]*len(volumes_path_list)\n",
    "labels_list = [[]]*len(volumes_path_list)\n",
    "for i, p in enumerate(volumes_path_list):\n",
    "    _, fname = os.path.split(p)\n",
    "    info = fname.split('.')[0:-1]\n",
    "    info = ''.join(info)\n",
    "    info = info.split('_')\n",
    "    fid = int(info[0])\n",
    "    vol_id = info[1]\n",
    "    sin_anomalias = int(info[2])\n",
    "    condensacion = int(info[3])\n",
    "    nodulos = int(info[4])\n",
    "    quistes = int(info[5])\n",
    "    samples_path_list[ fid - 1 ] = p\n",
    "    labels_list[ fid - 1 ] = [sin_anomalias, condensacion, nodulos, quistes]\n",
    "labels_as_classes = torch.LongTensor([get_class_id(l) for l in labels_list])\n",
    "\n",
    "negative_compatibles_dict = determine_negative_compatibles(PROXIMITY_VECTOR_LABELS_FOR_TRAINING)\n",
    "\n",
    "train_frac = 0.8\n",
    "x_train, x_test, y_train, y_test = train_test_split(samples_path_list, labels_as_classes, train_size=train_frac, stratify=labels_as_classes, random_state=seed)\n",
    "\n",
    "triplet_training_set = ProximityCTEmbeddingTripletDataset(x_train, y_train, preprocessed=True)\n",
    "triplet_validation_set = ProximityCTEmbeddingTripletDataset(x_test, y_test, preprocessed=True)\n",
    "\n",
    "print('Training set: count for each label')\n",
    "for label, count in enumerate(np.bincount(sorted(triplet_training_set.labels_list))):\n",
    "    print(f'label {label}: {count}')\n",
    "print('Training set size:', len(triplet_training_set))\n",
    "print()\n",
    "print('Validation set: count for each label')\n",
    "for label, count in enumerate(np.bincount(sorted(triplet_validation_set.labels_list))):\n",
    "    print(f'label {label}: {count}')\n",
    "print('Validation set size:', len(triplet_validation_set))\n",
    "\n",
    "def collate(batch):\n",
    "    batch = list(filter(lambda x:x is not None, batch))\n",
    "    return default_collate(batch)\n",
    "\n",
    "# balanced batch sampler: batch size is n_classes*n_samples. Each batch contains n_samples samples for n_classes different classes.\n",
    "sampler_n_classes = 2\n",
    "sampler_n_samples = 4\n",
    "sampler_batch_size = sampler_n_classes * sampler_n_samples\n",
    "train_batch_sampler = ProximityCTEmbeddingTripletDataset(triplet_training_set.labels_list, PROXIMITY_VECTOR_LABELS_FOR_TRAINING, n_classes=sampler_n_classes, n_samples=sampler_n_samples, multilabel=True)\n",
    "test_batch_sampler = ProximityCTEmbeddingTripletDataset(triplet_validation_set.labels_list, PROXIMITY_VECTOR_LABELS_FOR_TRAINING, n_classes=sampler_n_classes, n_samples=sampler_n_samples, multilabel=True)\n",
    "\n",
    "# Set up data loaders\n",
    "#n_classes = 2\n",
    "#batch_size = 8\n",
    "kwargs = {'num_workers': 0, 'pin_memory': True} if cuda else {}\n",
    "train_eval_loader = torch.utils.data.DataLoader(triplet_training_set, batch_size=sampler_batch_size, shuffle=False, **kwargs)\n",
    "test_eval_loader = torch.utils.data.DataLoader(triplet_validation_set, batch_size=sampler_batch_size, shuffle=False, **kwargs)\n",
    "triplet_train_loader = torch.utils.data.DataLoader(triplet_training_set, batch_sampler=train_batch_sampler, **kwargs)\n",
    "triplet_test_loader = torch.utils.data.DataLoader(triplet_validation_set, batch_sampler=test_batch_sampler, **kwargs)\n",
    "all_triplet_train_loader = TripletDataLoader(triplet_training_set, n_classes=sampler_n_classes, n_samples=sampler_n_samples, **kwargs)\n",
    "all_triplet_test_loader = TripletDataLoader(triplet_validation_set, n_classes=sampler_n_classes, n_samples=sampler_n_samples, **kwargs)\n",
    "\n",
    "\n",
    "margin = 0.2\n",
    "embedding_size = 128\n",
    "embedding_net = Proximity300x300(embedding_size=embedding_size)\n",
    "model = embedding_net\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "    \n",
    "loss_fn = OnlineTripletLoss(margin, SemihardNegativeTripletSelector(margin), negative_compatibles_dict, print_interval=10)\n",
    "lr = 1e-5\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, 50, gamma=0.1, last_epoch=-1)\n",
    "n_epochs = 50\n",
    "log_interval = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280458b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = [b for b in train_batch_sampler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d2c60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57aeece8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cbd9f9-e6ba-40d9-b1d5-5ae25358f65c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from trainer import Trainer\n",
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d%H%M%S') \n",
    "\n",
    "log_subdir = f\"slice_size=100x100 train_frac={train_frac} sampler={sampler_n_classes}x{sampler_n_samples} margin={margin} embedding_size={embedding_size} lr={lr} distance=euclidean_squared embedding_l2_norm=1 negative_mining=semihard batch_sampling=oversample_underrepresented_classes timestamp={timestamp}\"\n",
    "\n",
    "start_epoch = 0\n",
    "train_full_loader_switch = False\n",
    "metrics=[\n",
    "    AverageNonzeroTripletsMetric(), \n",
    "    TotalNonzeroTripletsMetric(), \n",
    "    Loss(),\n",
    "    #NDCG(proximity_vector_labels_dict, proximity_class_names),\n",
    "    #Recall(proximity_vector_labels_dict, proximity_class_names),\n",
    "    AllMetrics(PROXIMITY_VECTOR_LABELS_FOR_TRAINING, PROXIMITY_CLASS_NAMES),\n",
    "]\n",
    "\n",
    "trainer = Trainer(\n",
    "    triplet_train_loader, \n",
    "    triplet_test_loader,\n",
    "    train_eval_loader, \n",
    "    test_eval_loader, \n",
    "    all_triplet_train_loader,\n",
    "    all_triplet_test_loader,\n",
    "    model, \n",
    "    loss_fn, \n",
    "    optimizer, \n",
    "    scheduler, \n",
    "    n_epochs, \n",
    "    cuda, \n",
    "    log_interval,\n",
    "    TRIPLET_CHECKPOINTS_PATH,\n",
    "    os.path.join(DR2156_TRIPLET_TENSORBOARD_LOGS_DIR, log_subdir),\n",
    "    train_full_loader_switch,\n",
    "    metrics,\n",
    "    start_epoch,\n",
    ")\n",
    "\n",
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1136318d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aab9aec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4746a31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a2840e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2fcbff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af7d51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(\n",
    "    model.state_dict(),\n",
    "    os.path.join(TRIPLET_CHECKPOINTS_PATH, f'triplets_epoch={32}_val-loss={0.3881}_avg-nonzero-triplets={348588.0}.pth'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f32fa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import extract_embeddings\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "#BEST_MODEL_PATH = os.path.join(TRIPLET_CHECKPOINTS_PATH, 'triplets_val-loss_0.0467_20240424_050251.pth')\n",
    "\n",
    "#best_model = SynthDataEmbeddingModel(embedding_size=3).cuda()\n",
    "#best_model.load_state_dict(torch.load(BEST_MODEL_PATH))\n",
    "\n",
    "train_embeddings_tl, train_labels_tl = extract_embeddings(train_eval_loader, model)\n",
    "train_tsne = TSNE(n_components=2, perplexity=10, learning_rate=\"auto\", init=\"random\")\n",
    "train_embeddings_tsne = train_tsne.fit_transform(train_embeddings_tl.cpu().numpy())\n",
    "plot_embeddings(train_embeddings_tsne, train_labels_tl.numpy())\n",
    "\n",
    "test_embeddings_tl, test_labels_tl = extract_embeddings(test_eval_loader, model)\n",
    "val_tsne = TSNE(n_components=2, perplexity=5, learning_rate=\"auto\", init=\"random\")\n",
    "test_embeddings_tsne = val_tsne.fit_transform(test_embeddings_tl.cpu().numpy())\n",
    "plot_embeddings(test_embeddings_tsne, test_labels_tl.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb72b6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import extract_embeddings\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#BEST_MODEL_PATH = os.path.join(TRIPLET_CHECKPOINTS_PATH, 'triplets_val-loss_0.0467_20240424_050251.pth')\n",
    "\n",
    "#best_model = SynthDataEmbeddingModel(embedding_size=3).cuda()\n",
    "#best_model.load_state_dict(torch.load(BEST_MODEL_PATH))\n",
    "\n",
    "#train_embeddings_tl, train_labels_tl = extract_embeddings(train_eval_loader, model)\n",
    "train_pca = PCA(n_components=2)\n",
    "train_embeddings_pca = train_pca.fit_transform(train_embeddings_tl.cpu().numpy())\n",
    "plot_embeddings(train_embeddings_pca, train_labels_tl.numpy())\n",
    "\n",
    "#test_embeddings_tl, test_labels_tl = extract_embeddings(test_eval_loader, model)\n",
    "val_pca = PCA(n_components=2)\n",
    "test_embeddings_pca = val_pca.fit_transform(test_embeddings_tl.cpu().numpy())\n",
    "plot_embeddings(test_embeddings_pca, test_labels_tl.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ef0202",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import extract_embeddings\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "BEST_MODEL_PATH = os.path.join(TRIPLET_CHECKPOINTS_PATH, 'triplets_epoch=200_val-loss=38.9467_avg-nonzero-triplets=162.0.pth')\n",
    "\n",
    "best_model = SynthDataEmbeddingModel(embedding_size=8).cuda()\n",
    "best_model.load_state_dict(torch.load(BEST_MODEL_PATH))\n",
    "\n",
    "train_embeddings_tl, train_labels_tl = extract_embeddings(train_eval_loader, best_model)\n",
    "train_tsne = TSNE(n_components=2, perplexity=10, learning_rate=\"auto\", init=\"random\")\n",
    "train_embeddings_tsne = train_tsne.fit_transform(train_embeddings_tl.cpu().numpy())\n",
    "plot_embeddings(train_embeddings_tsne, train_labels_tl.numpy())\n",
    "\n",
    "test_embeddings_tl, test_labels_tl = extract_embeddings(test_eval_loader, best_model)\n",
    "val_tsne = TSNE(n_components=2, perplexity=5, learning_rate=\"auto\", init=\"random\")\n",
    "test_embeddings_tsne = val_tsne.fit_transform(test_embeddings_tl.cpu().numpy())\n",
    "plot_embeddings(test_embeddings_tsne, test_labels_tl.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297dd605",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import extract_embeddings\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#train_embeddings_tl, train_labels_tl = extract_embeddings(train_eval_loader, model)\n",
    "train_pca = PCA(n_components=2)\n",
    "train_embeddings_pca = train_pca.fit_transform(train_embeddings_tl.cpu().numpy())\n",
    "plot_embeddings(train_embeddings_pca, train_labels_tl.numpy())\n",
    "\n",
    "#test_embeddings_tl, test_labels_tl = extract_embeddings(test_eval_loader, model)\n",
    "val_pca = PCA(n_components=2)\n",
    "test_embeddings_pca = val_pca.fit_transform(test_embeddings_tl.cpu().numpy())\n",
    "plot_embeddings(test_embeddings_pca, test_labels_tl.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
