{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ea99a53-9589-4e99-8f99-2cfb15afbd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "# Add project root to PYTHONPATH\n",
    "project_root = os.getcwd()\n",
    "if str(project_root) not in sys.path and project_root.split('/')[-1] == 'chest_ct_retrieval':\n",
    "    sys.path.append(str(project_root))\n",
    "elif str(project_root) not in sys.path and project_root.split('/')[-1] == 'notebooks':\n",
    "    sys.path.append(str(os.path.normpath('/'.join(project_root.split('/')[:-1]))))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5624f831",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/backups/exports/dataChivo/jpmunoz/chest_ct_retrieval/training/trainer.py:42: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = GradScaler('cuda', enabled=self.use_amp)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d49db972671496c955c094c1eb595da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### EPOCH 1 START ###\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 71\u001b[39m\n\u001b[32m     49\u001b[39m p_metrics = load_metrics(cfg)\n\u001b[32m     51\u001b[39m trainer = Trainer(\n\u001b[32m     52\u001b[39m     train_loader=loaders[\u001b[33m\"\u001b[39m\u001b[33mtrain_triplet\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     53\u001b[39m     val_loader=loaders[\u001b[33m\"\u001b[39m\u001b[33mtest_triplet\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m     69\u001b[39m     start_epoch=\u001b[32m0\u001b[39m,\n\u001b[32m     70\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/net/backups/exports/dataChivo/jpmunoz/chest_ct_retrieval/training/trainer.py:66\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     64\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m### EPOCH \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.current_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m START ###\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# Train stage\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m train_loss, metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[38;5;28mself\u001b[39m.last_train_loss = train_loss\n\u001b[32m     68\u001b[39m \u001b[38;5;28mself\u001b[39m.scheduler.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/net/backups/exports/dataChivo/jpmunoz/chest_ct_retrieval/training/trainer.py:145\u001b[39m, in \u001b[36mTrainer.train_epoch\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    143\u001b[39m eval_loss_fn = OnlineTripletLoss(margin, SemihardNegativeTripletSelector(margin), negative_compatibles_dict, print_interval=\u001b[32m0\u001b[39m)\n\u001b[32m    144\u001b[39m pre_epoch_embeddings, labels = extract_embeddings(\u001b[38;5;28mself\u001b[39m.train_eval_loader, \u001b[38;5;28mself\u001b[39m.model)\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m true_loss_outputs, pre_epoch_triplets = \u001b[43meval_loss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpre_epoch_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_target\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdb_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpre_epoch_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdb_target\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    146\u001b[39m train_batches = \u001b[38;5;28mself\u001b[39m.train_full_loader.generate_batches_from_triplets(pre_epoch_triplets)\n\u001b[32m    147\u001b[39m n_train_batches = \u001b[38;5;28mmin\u001b[39m(\u001b[32m30\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.train_full_loader))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/net/backups/exports/dataChivo/jpmunoz/venvs/proximity/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/net/backups/exports/dataChivo/jpmunoz/venvs/proximity/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/net/backups/exports/dataChivo/jpmunoz/chest_ct_retrieval/losses/losses.py:120\u001b[39m, in \u001b[36mOnlineTripletLoss.forward\u001b[39m\u001b[34m(self, query_embeddings, query_target, db_embeddings, db_target)\u001b[39m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m query_embeddings.is_cuda \u001b[38;5;129;01mor\u001b[39;00m db_embeddings.is_cuda:\n\u001b[32m    117\u001b[39m     \u001b[38;5;66;03m#triplets = [t.cuda() for t in triplets]\u001b[39;00m\n\u001b[32m    118\u001b[39m     triplets = triplets.cuda()\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m ap_distances = (\u001b[43mdb_embeddings\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtriplets\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mdb_embeddings\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtriplets\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m).pow(\u001b[32m2\u001b[39m).sum(\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# .pow(.5)\u001b[39;00m\n\u001b[32m    121\u001b[39m an_distances = (db_embeddings[triplets[:, \u001b[32m0\u001b[39m]] - db_embeddings[triplets[:, \u001b[32m2\u001b[39m]]).pow(\u001b[32m2\u001b[39m).sum(\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# .pow(.5)\u001b[39;00m\n\u001b[32m    122\u001b[39m losses = F.relu(ap_distances - an_distances + \u001b[38;5;28mself\u001b[39m.margin)\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: out of memory\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from config.config import load_config\n",
    "from utils.seed import set_seed\n",
    "from training.setup import setup_training_run\n",
    "from training.environment import configure_environment\n",
    "from eval.metric_loader import load_metrics\n",
    "from training.data_setup import load_dataset, create_loaders\n",
    "from training.model_setup import initialize_model\n",
    "from training.trainer import Trainer\n",
    "\n",
    "\n",
    "cuda_available = torch.cuda.is_available()\n",
    "\n",
    "cfg = load_config(\"config/base.yaml\")\n",
    "\n",
    "set_seed(cfg[\"training\"][\"seed\"])\n",
    "\n",
    "run_dirs = setup_training_run(cfg[\"paths\"][\"dr2156\"][\"triplet_runs\"])\n",
    "\n",
    "checkpoints_dir = run_dirs[\"checkpoints\"]\n",
    "tensorboard_dir = run_dirs[\"logs\"]\n",
    "\n",
    "configure_environment(cfg)\n",
    "\n",
    "train_set, test_set, neg_compatibles = load_dataset(\n",
    "    cfg[\"paths\"][\"dr2156\"][\"preprocessed_300\"], \n",
    "    cfg[\"training\"][\"seed\"], \n",
    "    float(cfg[\"dataset\"][\"train_frac\"])\n",
    ")\n",
    "\n",
    "p_model, p_loss_fn, p_optimizer, p_scheduler = initialize_model(\n",
    "    embedding_size=int(cfg[\"model\"][\"embedding_size\"]),\n",
    "    margin=float(cfg[\"loss\"][\"margin\"]),\n",
    "    lr=float(cfg[\"training\"][\"optimizer\"][\"lr\"]),\n",
    "    weight_decay=float(cfg[\"training\"][\"optimizer\"][\"weight_decay\"]),\n",
    "    negative_compatibles_dict=neg_compatibles,\n",
    "    print_interval=int(cfg[\"logging\"][\"log_interval\"]),\n",
    "    cuda=cuda_available\n",
    ")\n",
    "\n",
    "loaders = create_loaders(\n",
    "    train_set,\n",
    "    test_set,\n",
    "    cfg[\"training\"][\"batch\"][\"n_classes\"],\n",
    "    cfg[\"training\"][\"batch\"][\"n_samples\"],\n",
    "    cuda_available\n",
    ")\n",
    "\n",
    "p_metrics = load_metrics(cfg)\n",
    "\n",
    "trainer = Trainer(\n",
    "    train_loader=loaders[\"train_triplet\"],\n",
    "    val_loader=loaders[\"test_triplet\"],\n",
    "    train_eval_loader=loaders[\"train_eval\"],\n",
    "    val_eval_loader=loaders[\"test_eval\"],\n",
    "    train_full_loader=loaders[\"all_triplet_train\"],\n",
    "    val_full_loader=loaders[\"all_triplet_test\"],\n",
    "    model=p_model,\n",
    "    loss_fn=p_loss_fn,\n",
    "    optimizer=p_optimizer,\n",
    "    scheduler=p_scheduler,\n",
    "    n_epochs=cfg[\"training\"][\"n_epochs\"],\n",
    "    cuda=cuda_available,\n",
    "    log_interval=cfg[\"logging\"][\"log_interval\"],\n",
    "    checkpoint_dir=checkpoints_dir,\n",
    "    tensorboard_logs_dir=tensorboard_dir,\n",
    "    train_full_loader_switch=cfg[\"training\"][\"train_full_loader_switch\"],\n",
    "    metrics=p_metrics,\n",
    "    start_epoch=0,\n",
    ")\n",
    "trainer.fit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
