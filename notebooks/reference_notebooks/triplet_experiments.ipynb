{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c220a39e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Paths & URLs\n",
    "\n",
    "import os\n",
    "\n",
    "# Enable CUDA stacktrace reporting for debugging\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "# Directorio base\n",
    "#PATH_BASE = '/content/drive/MyDrive/proximity'\n",
    "PATH_BASE = 'C:\\\\Users\\\\User\\\\Documents\\\\Proyecto Proximity'\n",
    "\n",
    "# Data release actual\n",
    "DR70_PATH = os.path.join(PATH_BASE, 'DR70')\n",
    "DR70_CT_PATH = os.path.join(DR70_PATH, 'datalake_sorted')\n",
    "DR70_LABELS_PATH = os.path.join(DR70_PATH, 'labels.csv')\n",
    "\n",
    "\n",
    "# CTs in Nibabel format\n",
    "CT_NIBABEL_PATH = os.path.join(PATH_BASE, 'DR70', 'CTs')\n",
    "\n",
    "# Embeddings visuales de CTs\n",
    "#CT_EMBEDDINGS_PATH = DATA_RELEASE_PATH + '/visual_embeddings'\n",
    "\n",
    "\n",
    "# Etiquetas de los CTs del data release actual\n",
    "#CT_LABELS_CSV_PATH = DATA_RELEASE_PATH + '/labels.csv'\n",
    "\n",
    "# Data release (CTs + etiquetas) organizados en un DataFrame\n",
    "#CT_DATASET_DF_HDF_PATH = os.path.join(PATH_BASE, 'dataset_df.h5')\n",
    "#CT_DATASET_DF_PICKLE_PATH = os.path.join(PATH_BASE, 'dataset_df.pickle')\n",
    "\n",
    "# URLs de modelos visuales\n",
    "#RESNET18_URL = 'microsoft/resnet-18'\n",
    "\n",
    "# Path que contiene los resnet50 embeddings de CTs del data release actual\n",
    "#CT_RESNET18_EMBEDDINGS_PATH = os.path.join(DR70_PATH, 'visual_embeddings', 'resnet18')\n",
    "#CT_RESNET18_EMBEDDINGS_PATH = os.path.join(DR70_PATH, 'visual_embeddings', 'resnet18', 'reshaped_averaged')\n",
    "\n",
    "# Path de modelos entrenados en base a tripletas\n",
    "TRIPLET_MODELS_PATH = os.path.join(PATH_BASE, 'retrieval_models', 'triplets')\n",
    "TRIPLET_CHECKPOINTS_PATH = os.path.join(PATH_BASE, 'retrieval_models', 'triplets', 'checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0389c01b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataframe_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 35\u001b[0m\n\u001b[0;32m     29\u001b[0m     train_df, val_df, test_df \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msplit(\n\u001b[0;32m     30\u001b[0m         labels_df\u001b[38;5;241m.\u001b[39msample(frac\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m31\u001b[39m),\n\u001b[0;32m     31\u001b[0m         [train_size, train_size \u001b[38;5;241m+\u001b[39m val_size]\n\u001b[0;32m     32\u001b[0m     )\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (train_df, val_df, test_df)\n\u001b[1;32m---> 35\u001b[0m train_df, val_df, test_df \u001b[38;5;241m=\u001b[39m split_dataset(DR70_LABELS_PATH)\n",
      "Cell \u001b[1;32mIn[2], line 16\u001b[0m, in \u001b[0;36msplit_dataset\u001b[1;34m(dataframe_path)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msplit_dataset\u001b[39m(dataframe_path):\n\u001b[1;32m---> 16\u001b[0m     labels_df \u001b[38;5;241m=\u001b[39m read_labels_csv(dataframe_path)\n\u001b[0;32m     18\u001b[0m     train_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.8\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(labels_df))\n\u001b[0;32m     19\u001b[0m     val_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.2\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(labels_df))\n",
      "Cell \u001b[1;32mIn[2], line 8\u001b[0m, in \u001b[0;36mread_labels_csv\u001b[1;34m(csv_path)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_labels_csv\u001b[39m(csv_path):\n\u001b[0;32m      7\u001b[0m     labels_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\n\u001b[1;32m----> 8\u001b[0m         dataframe_path, \n\u001b[0;32m      9\u001b[0m         header\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \n\u001b[0;32m     10\u001b[0m         index_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \n\u001b[0;32m     11\u001b[0m         dtype\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mct\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcondensacion\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnodulos\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquistes\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mbool\u001b[39m}\n\u001b[0;32m     12\u001b[0m     )\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m labels_df\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataframe_path' is not defined"
     ]
    }
   ],
   "source": [
    "# DATASET LOADING & PREPARATION\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def read_labels_csv(csv_path):\n",
    "    labels_df = pd.read_csv(\n",
    "        dataframe_path, \n",
    "        header=0, \n",
    "        index_col=0, \n",
    "        dtype={'ct': str, 'condensacion': int, 'nodulos': int, 'quistes': int}\n",
    "    )\n",
    "    return labels_df\n",
    "\n",
    "def split_dataset(dataframe_path):\n",
    "    labels_df = read_labels_csv(dataframe_path)\n",
    "\n",
    "    train_size = int(0.8 * len(labels_df))\n",
    "    val_size = int(0.2 * len(labels_df))\n",
    "    test_size = int(0.0 * len(labels_df))\n",
    "\n",
    "    '''\n",
    "    If necessary, adjust the size of the training set when the sum of \n",
    "    the sizes of the three sets differs from the total dataset size.\n",
    "    '''\n",
    "    size_diff = len(labels_df) - train_size - val_size - test_size\n",
    "    train_size += size_diff\n",
    "\n",
    "    train_df, val_df, test_df = np.split(\n",
    "        labels_df.sample(frac=1.0, random_state=31),\n",
    "        [train_size, train_size + val_size]\n",
    "    )\n",
    "    return (train_df, val_df, test_df)\n",
    "\n",
    "train_df, val_df, test_df = split_dataset(DR70_LABELS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b48ba07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56, 4) (14, 4) (0, 4)\n"
     ]
    }
   ],
   "source": [
    "print(train_df.shape, val_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbe87a37",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsize_diff = len(dataset) - train_size - val_size - test_size\\ntrain_size += size_diff\\n\\n# Use random_split to split the dataset\\ntrain_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\\n\\n\\ntraining_sampler = RandomSampler(train_dataset, replacement=False, generator=torch.Generator().manual_seed(42))\\nbatch_sampler = BatchSampler(sampler=training_sampler, batch_size=8, drop_last=True)\\n\\ntraining_dataloader = torch.utils.data.DataLoader(\\n    train_dataset, \\n    batch_size=8, \\n    shuffle=True, \\n    generator=torch.Generator().manual_seed(42)\\n)\\nct_embedding_model = CTModel()\\ntriplet_model = TripletModel(model=ct_embedding_model, mode=\\'HN\\', seed=42)\\n\\ne = dataset.get_updated_embeddings(triplet_model, torch.device(\"cuda\"))\\n\\nprint(dataset.get_embeddings([0]))\\n\\n\\ntraining_sampler = RandomSampler(train_dataset, replacement=False, generator=torch.Generator().manual_seed(42))\\nval_sampler = RandomSampler(val_dataset, replacement=False, generator=torch.Generator().manual_seed(42))\\n\\nprint(train_dataset[:][0])\\nprint(val_dataset[:][0])\\nprint([e for e in training_sampler])\\nprint([e for e in val_sampler])\\n\\n\\n# You can now create data loaders for each split if needed\\n\\n# Create a RandomSampler with seed 42 and no replacement\\ntraining_sampler = RandomSampler(train_dataset, replacement=False, generator=torch.Generator().manual_seed(42))\\nbatch_sampler = BatchSampler(sampler=training_sampler, batch_size=8, drop_last=True)\\n        \\n\\n\\nimport time\\nlim = 99\\nfor batch in batch_sampler:\\n    if lim == 0:\\n        break\\n    lim -= 1\\n    print(\\'\\')\\n    print(\\'Batch:\\', batch)  # Do whatever you want with the batch data\\n    ids = list()\\n    sample_data = list()\\n    labels_data = list()\\n    idx, sample, labels = dataset[batch]\\n    print(\\'Labels:\\', labels_data)\\n    positive, negative = dataset.get_batch_positive_negative_pairs(batch)\\n    print(\\'Positive pairs:\\', positive)\\n    print(\\'Negative pairs:\\', negative)\\n    print(\\'sample shape\\', sample.shape)\\n    time.sleep(0.5) # just to make the logging and prints to output in the correct order!\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchio as tio\n",
    "from torch.utils.data import Dataset, random_split, RandomSampler, BatchSampler\n",
    "from scipy.spatial.distance import hamming\n",
    "import random\n",
    "import logging\n",
    "\n",
    "# Set seeds\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_dir, labels_csv_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self.labels_df = pd.read_csv(\n",
    "            labels_csv_dir, \n",
    "            header=0, \n",
    "            index_col=0, \n",
    "            dtype={'CT': str, 'condensacion': int, 'nodulos': int, 'quistes': int}\n",
    "        )\n",
    "        \n",
    "        '''\n",
    "        delete rows from the dataset that don't have a corresponding embedding--they\n",
    "        may not exist because they were invalid or their CTs too short to be processed by \n",
    "        the visual encoder)\n",
    "        '''\n",
    "        to_be_deleted_ids = list()\n",
    "        for idx, ct_id in zip(self.labels_df.index, self.labels_df['CT']):\n",
    "            file_path = os.path.join(self.data_dir, f\"{ct_id}.npy\")\n",
    "            if not os.path.exists(file_path):\n",
    "                to_be_deleted_ids.append(idx)\n",
    "                logging.warning(f\"Data point with dataset_id={idx} will be discarded because its corresponding visual embedding file does not exist: {file_path}\")\n",
    "        if len(to_be_deleted_ids) > 0:\n",
    "            self.labels_df = self.labels_df.drop(index=to_be_deleted_ids)\n",
    "            self.labels_df = self.labels_df.reset_index(drop=True)\n",
    "        self.labels_df['embedding'] = None\n",
    "        #print(self.labels_df.head())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #print('__getitem__',idx)\n",
    "        if hasattr(idx, '__iter__'):\n",
    "            return self.__getitems__(idx)\n",
    "        else:\n",
    "            return self.__getitems__([idx])\n",
    "\n",
    "    def __getitems__(self, ids):\n",
    "        ids_list = list()\n",
    "        samples_list = list()\n",
    "        labels_list = list()\n",
    "        for idx in ids:\n",
    "            content, labels = self.load_data(idx)\n",
    "            ids_list.append(idx)\n",
    "            samples_list.append(content)\n",
    "            labels_list.append(labels)\n",
    "        ids_list = torch.tensor(ids)\n",
    "        samples_list = torch.stack(samples_list)\n",
    "        labels_list = torch.stack(labels_list)\n",
    "        return (ids_list.detach().clone(), samples_list.detach().clone(), labels_list.detach().clone())\n",
    "\n",
    "    \n",
    "    def get_updated_embeddings(self, torch_model, device=None):\n",
    "        embeddings = list()\n",
    "        with torch.inference_mode():\n",
    "            for i, data_sample in self.labels_df.iterrows():\n",
    "                data_input, data_labels = self.load_data(i)\n",
    "                if data_input.shape[0] != 1:\n",
    "                    data_input = data_input.unsqueeze(0)\n",
    "                if device:\n",
    "                    data_output = torch_model(data_input.to(device))\n",
    "                else:\n",
    "                    data_output = torch_model(data_input)\n",
    "                embeddings.append(data_output)\n",
    "        self.labels_df['embedding'] = embeddings\n",
    "        return torch.stack(embeddings)\n",
    "    \n",
    "    def get_embeddings(self, ids):\n",
    "        if not hasattr(ids, '__iter__'):\n",
    "            ids = [ids]\n",
    "        selected_data_samples = self.labels_df.iloc[ids]\n",
    "        #print('selected_data_samples[\\'embedding\\'].shape:', selected_data_samples['embedding'].shape)\n",
    "        #print('selected_data_samples[\\'embedding\\']:', selected_data_samples['embedding'])\n",
    "        selected_embeddings = torch.stack(selected_data_samples['embedding'].to_list())\n",
    "        return selected_embeddings\n",
    "    \n",
    "    def load_data(self, idx):\n",
    "        #print('load_data',idx)\n",
    "        ct_id = self.labels_df['CT'].iloc[idx]\n",
    "        file_path = os.path.join(self.data_dir, f\"{ct_id}.npy\")\n",
    "        # WARN: some embedding files associated to CTs in the dataset may not exist: the corresponding CTs are not valid as they contained too few slices to be processed by the visual encoder\n",
    "        if os.path.exists(file_path):\n",
    "            sample_content = np.load(file_path)\n",
    "            if sample_content.shape[0] == 1:\n",
    "                sample_content = sample_content.squeeze(0)\n",
    "            sample_content = torch.from_numpy(sample_content).detach().clone()\n",
    "            sample_labels = np.array(self.labels_df.iloc[idx].iloc[1:4], dtype=int)\n",
    "            sample_labels = torch.from_numpy(sample_labels).detach().clone()\n",
    "            return (sample_content, sample_labels)\n",
    "        \n",
    "    def get_batch_positive_negative_pairs(self, ids_list):\n",
    "        logging.info(f\"Generating triplets for batch {ids_list}\")\n",
    "        assert(len(ids_list) > 0)\n",
    "        positives_dict = dict() # positives_dict := { anchor_id : list_of_positives_ids }\n",
    "        negatives_dict = dict() # negatives_dict _= { anchor_id : (ids not contained in list_of_positives_ids) }\n",
    "        positive_candidates_label_vectors = list(np.array(self.labels_df.iloc[ids_list].iloc[:,1:4], dtype=int))\n",
    "        \n",
    "        for j in ids_list:\n",
    "            positives_list = list()\n",
    "            negatives_list = list()\n",
    "            anchor_label_vector = list(np.array(self.labels_df.iloc[j].iloc[1:4], dtype=int))\n",
    "            for i, positive_candidate_label_vector in zip(ids_list, positive_candidates_label_vectors):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                distance = hamming(anchor_label_vector, positive_candidate_label_vector)\n",
    "                if distance == 0.0:\n",
    "                    positives_list.append(i)\n",
    "                else:\n",
    "                    negatives_list.append(i)\n",
    "            positives_dict[j] = positives_list\n",
    "            negatives_dict[j] = negatives_list\n",
    "        \n",
    "        positive_pairs = list() # positive_pairs := list of pairs [anchor_id, positive_example_id]\n",
    "        negative_candidates = dict() # negative_candidates := dict of form {anchor_id : [negative_example_1, negative_example_2, ...]]}\n",
    "        for k in positives_dict:\n",
    "            if len(positives_dict[k]) > 0:\n",
    "                positive_pairs.append([k, random.choice(positives_dict[k])])\n",
    "            else:\n",
    "                logging.info(f\"Could not construct a positive pair for anchor ID {k} in this batch. Skipping...\")\n",
    "                continue\n",
    "            if len(negatives_dict[k]) > 0:\n",
    "                negative_candidates[k] = negatives_dict[k]\n",
    "            else:\n",
    "                logging.info(f\"Could not construct a negative pair for anchor ID {k} in this batch. Skipping...\")\n",
    "                continue\n",
    "        return positive_pairs, negative_candidates\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "# Create an instance of the dataset\n",
    "dataset = CustomDataset(CT_RESNET18_EMBEDDINGS_PATH, DR70_LABELS_PATH)\n",
    "\n",
    "# Define the sizes of train, validation, and test sets\n",
    "train_size = int(0.7 * len(dataset))  # 70% of the data for training\n",
    "val_size = int(0.15 * len(dataset))     # 15% of the data for validation\n",
    "test_size = len(dataset) - train_size - val_size  # Remaining data for testing\n",
    "\n",
    "'''\n",
    "#If necessary, adjust the size of the training set when the sum of \n",
    "#the sizes of the three sets differs from the total dataset size.\n",
    "'''\n",
    "size_diff = len(dataset) - train_size - val_size - test_size\n",
    "train_size += size_diff\n",
    "\n",
    "# Use random_split to split the dataset\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "\n",
    "training_sampler = RandomSampler(train_dataset, replacement=False, generator=torch.Generator().manual_seed(42))\n",
    "batch_sampler = BatchSampler(sampler=training_sampler, batch_size=8, drop_last=True)\n",
    "\n",
    "training_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=8, \n",
    "    shuffle=True, \n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "ct_embedding_model = CTModel()\n",
    "triplet_model = TripletModel(model=ct_embedding_model, mode='HN', seed=42)\n",
    "\n",
    "e = dataset.get_updated_embeddings(triplet_model, torch.device(\"cuda\"))\n",
    "\n",
    "print(dataset.get_embeddings([0]))\n",
    "\n",
    "\n",
    "training_sampler = RandomSampler(train_dataset, replacement=False, generator=torch.Generator().manual_seed(42))\n",
    "val_sampler = RandomSampler(val_dataset, replacement=False, generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "print(train_dataset[:][0])\n",
    "print(val_dataset[:][0])\n",
    "print([e for e in training_sampler])\n",
    "print([e for e in val_sampler])\n",
    "\n",
    "\n",
    "# You can now create data loaders for each split if needed\n",
    "\n",
    "# Create a RandomSampler with seed 42 and no replacement\n",
    "training_sampler = RandomSampler(train_dataset, replacement=False, generator=torch.Generator().manual_seed(42))\n",
    "batch_sampler = BatchSampler(sampler=training_sampler, batch_size=8, drop_last=True)\n",
    "        \n",
    "\n",
    "\n",
    "import time\n",
    "lim = 99\n",
    "for batch in batch_sampler:\n",
    "    if lim == 0:\n",
    "        break\n",
    "    lim -= 1\n",
    "    print('')\n",
    "    print('Batch:', batch)  # Do whatever you want with the batch data\n",
    "    ids = list()\n",
    "    sample_data = list()\n",
    "    labels_data = list()\n",
    "    idx, sample, labels = dataset[batch]\n",
    "    print('Labels:', labels_data)\n",
    "    positive, negative = dataset.get_batch_positive_negative_pairs(batch)\n",
    "    print('Positive pairs:', positive)\n",
    "    print('Negative pairs:', negative)\n",
    "    print('sample shape', sample.shape)\n",
    "    time.sleep(0.5) # just to make the logging and prints to output in the correct order!\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "50c76500-77ae-41b1-98a5-6932d2e18d32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([66])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset[2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfb669f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def recall(labels_gt, labels_prediction):\n",
    "    \"\"\"\n",
    "    set of ground truth labels and list of prediction lists. \n",
    "    This is an independent of k way to calculate the recall.\n",
    "    \"\"\"\n",
    "    n = len(labels_prediction)\n",
    "    recall = 0\n",
    "    for i in range(n):\n",
    "        recall += len(set(labels_gt) & set(labels_prediction[i]))/len(labels_gt)\n",
    "    return recall/n\n",
    "\n",
    "def get_batch_data(data, index, size):\n",
    "    \"\"\"\n",
    "    For minibatch training\n",
    "    \"\"\"\n",
    "    column_1 = []\n",
    "    column_2 = []\n",
    "    for i in range(index, index + size):\n",
    "        line = data[i]\n",
    "        # anchor image\n",
    "        column_1.append(int(line[0]))\n",
    "        # positive image\n",
    "        column_2.append(int(line[1]))\n",
    "    return np.array(column_1), np.array(column_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38cfe49c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import ConvNextImageProcessor, ResNetForImageClassification, ResNetConfig\n",
    "import os\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from scipy.ndimage import zoom\n",
    "from torchvision import models, utils\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "class NiftiLoader:\n",
    "    def __init__(self, folder_path, ct_target_depth=384, resize_strategy='nn_interpolation'):\n",
    "        self.folder_path = folder_path\n",
    "        self.ct_target_depth = ct_target_depth\n",
    "        self.file_list = [file_name for file_name in os.listdir(self.folder_path) if file_name.endswith(\".nii.gz\")]\n",
    "        self.resize_strategy = resize_strategy\n",
    "    \n",
    "    def __iter__(self):\n",
    "        self.current_idx = 0\n",
    "        return self\n",
    "\n",
    "    def get_ct_id_dict(self):\n",
    "        ct_id_dict = dict((file_name.split('.nii.gz')[0], i) for (i, file_name) in enumerate(self.file_list))\n",
    "        return ct_id_dict\n",
    "\n",
    "    def get_ct_info(self, ct_id):\n",
    "        file_name = self.file_list[ct_id]\n",
    "        file_path = os.path.join(self.folder_path, file_name)\n",
    "\n",
    "        # Load the NIfTI file\n",
    "        nifti_image = nib.load(file_path)\n",
    "        return nifti_image.header\n",
    "\n",
    "    def _fix_depth(self, pixel_ndarray):\n",
    "        if pixel_ndarray.shape[0] < self.ct_target_depth:\n",
    "            zoom_factor = self.ct_target_depth / pixel_ndarray.shape[0]\n",
    "            frame_diff = self.ct_target_depth - pixel_ndarray.shape[0]\n",
    "            x_size = pixel_ndarray.shape[1]\n",
    "            y_size = pixel_ndarray.shape[2]\n",
    "            if self.resize_strategy == \"zero_padding\":\n",
    "                pixel_ndarray = np.append(pixel_ndarray, np.zeros(shape=(frame_diff, x_size, y_size)), axis=0)\n",
    "            elif self.resize_strategy == \"nn_interpolation\":\n",
    "                pixel_ndarray = zoom(pixel_ndarray, (zoom_factor, 1, 1), order=0)\n",
    "            else:\n",
    "                raise ValueError(f\"Resize strategy '{self.resize_strategy}' not supported. Allowed values are 'zero_padding' and 'nn_interpolation'.\")\n",
    "        else:\n",
    "            fixed_pixel_ndarray = pixel_ndarray.astype(np.int16)[0:self.ct_target_depth]\n",
    "        return fixed_pixel_ndarray\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.current_idx < len(self.file_list):\n",
    "            file_name = self.file_list[self.current_idx]\n",
    "            file_path = os.path.join(self.folder_path, file_name)\n",
    "\n",
    "            # Extract CT_ID from the file name\n",
    "            ct_id = file_name.split('.nii.gz')[0]\n",
    "\n",
    "            # Load the NIfTI file\n",
    "            nifti_image = nib.load(file_path)\n",
    "\n",
    "            # Access the 3D numpy array from the NIfTI image\n",
    "            ct_volume = nifti_image.get_fdata()\n",
    "\n",
    "            ct_volume = self._fix_depth(ct_volume)\n",
    "\n",
    "            # Increment the index for the next iteration\n",
    "            self.current_idx += 1\n",
    "\n",
    "            # Return the tuple (CT_ID, CT_3D_array)\n",
    "            return ct_id, ct_volume\n",
    "        else:\n",
    "            raise StopIteration\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx < len(self.file_list):\n",
    "            file_name = self.file_list[idx]\n",
    "            file_path = os.path.join(self.folder_path, file_name)\n",
    "\n",
    "            # Extract CT_ID from the file name\n",
    "            ct_id = file_name.split('.nii.gz')[0]\n",
    "\n",
    "            # Load the NIfTI file\n",
    "            nifti_image = nib.load(file_path)\n",
    "\n",
    "            # Access the 3D numpy array from the NIfTI image\n",
    "            ct_volume = nifti_image.get_fdata()\n",
    "\n",
    "            ct_volume = self._fix_depth(ct_volume)\n",
    "\n",
    "            # Return the tuple (CT_ID, CT_3D_array)\n",
    "            return ct_id, ct_volume\n",
    "        else:\n",
    "            raise IndexError(\"Index out of range\")\n",
    "\n",
    "\n",
    "class ResNetImagePreprocessor(nn.Module):\n",
    "    def __init__(self, ct_shape):\n",
    "        super().__init__()\n",
    "        self.image_preprocessor = ConvNextImageProcessor(\n",
    "            do_resize=False,\n",
    "            do_rescale=True,\n",
    "            rescale_factor=1/255,\n",
    "            do_normalize=True,\n",
    "            image_mean=[0.485, 0.456, 0.406],\n",
    "            image_std=[0.229, 0.224, 0.225],\n",
    "        )\n",
    "        self.n_slices = ct_shape[0]\n",
    "        self.slice_h = ct_shape[1]\n",
    "        self.slice_w = ct_shape[2]\n",
    "        preprocessed_slice_thrices = torch.empty((0, self.n_slices // 3, self.slice_h, self.slice_w), dtype=torch.float32)\n",
    "        self.register_buffer('preprocessed_slice_thrices', preprocessed_slice_thrices, persistent=False)\n",
    "        preprocessed_img = torch.empty((3, self.slice_h, self.slice_w), dtype=torch.float32)\n",
    "        self.register_buffer('preprocessed_img', preprocessed_img, persistent=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for ct in x:\n",
    "            for i in range(self.n_slices // 3):\n",
    "                threechannel_img = ct[ 3*i : 3*i + 3 ]\n",
    "                print(self.preprocessed_img)\n",
    "                preprocess_output = self.image_preprocessor.preprocess(threechannel_img)['pixel_values']\n",
    "                self.preprocessed_img = torch.tensor(np.array(preprocess_output)).detach().clone()\n",
    "                print(self.preprocessed_img.shape)\n",
    "                print(self.preprocessed_img)\n",
    "                self.preprocessed_img = self.preprocessed_img.unsqueeze(0)\n",
    "                self.preprocessed_slice_thrices = torch.cat((self.preprocessed_slice_thrices, self.preprocessed_img), 0)\n",
    "        return self.preprocessed_slice_thrices\n",
    "\n",
    "class ResNetCTFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Load the ResNet model and modify the its configuration to make it output its hidden states\n",
    "        resnet_config = ResNetConfig.from_pretrained(\"microsoft/resnet-18\", output_hidden_states=True)\n",
    "        self.resnet_model = ResNetForImageClassification(resnet_config)\n",
    "        resnet_hidden_states = torch.empty((0, 512, 128, 6, 6), dtype=torch.float32)\n",
    "        self.register_buffer('resnet_hidden_states', resnet_hidden_states, persistent=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        n_images = x.shape[0]\n",
    "        for img in range(n_images):\n",
    "            resnet_hidden_states = torch.cat((resnet_hidden_states, self.resnet_model(preprocessed_img)['hidden_states'][-1]), 0)\n",
    "        return resnet_hidden_states\n",
    "\n",
    "class CTModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CTModel, self).__init__()\n",
    "        \n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "        self.features = nn.Sequential(*(list(resnet.children())[:-2]))\n",
    "\n",
    "        resnet_config = ResNetConfig.from_pretrained(\"microsoft/resnet-18\", output_hidden_states=True)\n",
    "        self.resnet_model = ResNetForImageClassification(resnet_config)\n",
    "\n",
    "        #conv input torch.Size([1,134,512,14,14])\n",
    "        self.reducingconvs = nn.Sequential(\n",
    "            nn.Conv3d(134, 64, kernel_size = (3,3,3), stride=(3,1,1), padding=0),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv3d(64, 32, kernel_size = (3,3,3), stride=(3,1,1), padding=0),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv3d(32, 16, kernel_size = (3,2,2), stride=(3,2,2), padding=0),\n",
    "            nn.ReLU())\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(16*18*6*6, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            nn.Linear(512, 256), \n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            nn.Linear(256, 128))\n",
    "\n",
    "        def forward(self, x):\n",
    "            shape = list(x.size())\n",
    "            #example shape: [1,134,3,420,420]\n",
    "            #example shape: [2,134,3,420,420]\n",
    "            batch_size = int(shape[0])\n",
    "            x = x.view(batch_size*134,3,512,512)\n",
    "            x = self.features(x)\n",
    "            x = x.view(batch_size,134,512,16,16)\n",
    "            x = self.reducingconvs(x)\n",
    "            #output is shape [batch_size, 16, 18, 6, 6]\n",
    "            x = x.view(batch_size, 16*18*6*6)\n",
    "            x = self.fc(x)\n",
    "            return x\n",
    "        \n",
    "        # Convolutional Layers\n",
    "        self.conv1 = nn.Sequential( \n",
    "            nn.Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(3, 1, 1), padding=0),\n",
    "            nn.BatchNorm3d(256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv2 = nn.Sequential( \n",
    "            nn.Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(3, 1, 1), padding=0),\n",
    "            nn.BatchNorm3d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv3 = nn.Sequential( \n",
    "            nn.Conv3d(128, 64, kernel_size=(3, 2, 2), stride=(3, 2, 2), padding=0),\n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*4*6*6, 1024)\n",
    "        )\n",
    "\n",
    "            \n",
    "    def old_forward(self, x):\n",
    "\n",
    "        # ResNet\n",
    "        x = self.resnet(x)\n",
    "        \n",
    "        # Convolutions\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "3263c2ad-47df-422f-9439-edbe3d0eb543",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0000,  0.0000, -2.1103,  ..., -2.1365, -2.1140, -2.0976],\n",
      "         [-2.1154, -2.1440, -2.1242,  ..., -2.1486, -2.1064, -2.1364],\n",
      "         [-2.1331, -2.0919, -2.0996,  ..., -2.1276, -2.1228, -2.1061],\n",
      "         ...,\n",
      "         [-2.1036, -2.1358, -2.1417,  ..., -2.1247, -2.0925, -2.1146],\n",
      "         [-2.0729, -2.1120, -2.0932,  ..., -2.1170, -2.1351, -2.1147],\n",
      "         [-2.1249, -2.0949, -2.1047,  ..., -2.1516, -2.1088, -2.1403]],\n",
      "\n",
      "        [[-2.0411, -2.0560, -2.0577,  ..., -2.0542, -2.0417, -2.0401],\n",
      "         [-2.0028, -2.0176, -2.0425,  ..., -2.0406, -2.0270, -2.0252],\n",
      "         [-2.0171, -2.0501, -2.0557,  ..., -2.0297, -2.0359, -2.0082],\n",
      "         ...,\n",
      "         [-2.0195, -2.0319, -2.0438,  ..., -2.0374, -2.0019, -2.0422],\n",
      "         [-2.0233, -2.0393, -2.0251,  ..., -2.0455, -2.0344, -2.0686],\n",
      "         [-2.0587, -2.0272, -2.0577,  ..., -2.0522, -2.0340, -2.0501]],\n",
      "\n",
      "        [[-1.8047, -1.8476, -1.8089,  ..., -1.8045, -1.7889, -1.8189],\n",
      "         [-1.8062, -1.8002, -1.8065,  ..., -1.7973, -1.7645, -1.8298],\n",
      "         [-1.8173, -1.8098, -1.8083,  ..., -1.8047, -1.8057, -1.8112],\n",
      "         ...,\n",
      "         [-1.7728, -1.7749, -1.7983,  ..., -1.8107, -1.7723, -1.7928],\n",
      "         [-1.7868, -1.7891, -1.7858,  ..., -1.7748, -1.8384, -1.7818],\n",
      "         [-1.7803, -1.7951, -1.8063,  ..., -1.8199, -1.8297, -1.7895]]],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 3, 512, 512])\n",
      "tensor([[[[-2.1352, -2.1290, -2.1381,  ..., -2.1218, -2.1102, -2.1337],\n",
      "          [-2.1073, -2.1397, -2.0868,  ..., -2.1066, -2.1230, -2.1149],\n",
      "          [-2.1146, -2.1448, -2.1160,  ..., -2.1433, -2.1356, -2.0931],\n",
      "          ...,\n",
      "          [-2.1137, -2.1103, -2.1450,  ..., -2.1231, -2.1084, -2.1335],\n",
      "          [-2.1264, -2.1103, -2.1296,  ..., -2.1158, -2.1303, -2.1160],\n",
      "          [-2.1043, -2.0790, -2.1549,  ..., -2.1179, -2.1245, -2.1048]],\n",
      "\n",
      "         [[-2.0154, -2.0281, -2.0363,  ..., -2.0409, -2.0259, -2.0582],\n",
      "          [-2.0391, -2.0130, -2.0482,  ..., -2.0383, -2.0125, -2.0534],\n",
      "          [-2.0205, -2.0275, -2.0314,  ..., -2.0331, -2.0219, -2.0181],\n",
      "          ...,\n",
      "          [-2.0181, -2.0230, -2.0568,  ..., -2.0334, -2.0550, -2.0734],\n",
      "          [-2.0541, -2.0514, -1.9999,  ..., -2.0273, -2.0356, -2.0206],\n",
      "          [-2.0209, -2.0324, -2.0232,  ..., -2.0283, -2.0209, -2.0191]],\n",
      "\n",
      "         [[-1.8161, -1.8269, -1.8324,  ..., -1.7712, -1.8120, -1.7940],\n",
      "          [-1.7868, -1.8161, -1.7984,  ..., -1.8297, -1.8084, -1.8083],\n",
      "          [-1.8285, -1.7954, -1.7863,  ..., -1.7843, -1.7969, -1.7887],\n",
      "          ...,\n",
      "          [-1.8326, -1.8205, -1.8058,  ..., -1.8034, -1.8036, -1.8057],\n",
      "          [-1.8055, -1.8197, -1.7990,  ..., -1.7894, -1.8426, -1.7861],\n",
      "          [-1.7880, -1.7959, -1.8240,  ..., -1.7726, -1.8247, -1.8038]]]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument tensors in method wrapper_CUDA_cat)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[127], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m ct_embedding_model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      7\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 9\u001b[0m out \u001b[38;5;241m=\u001b[39m ct_embedding_model(x)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(out\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[126], line 185\u001b[0m, in \u001b[0;36mCTModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    183\u001b[0m \n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# ResNet\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresnet(x)\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;66;03m# Convolutions\u001b[39;00m\n\u001b[0;32m    188\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[126], line 125\u001b[0m, in \u001b[0;36mResNetImagePreprocessor.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocessed_img)\n\u001b[0;32m    124\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocessed_img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocessed_img\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 125\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocessed_slice_thrices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocessed_slice_thrices, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocessed_img), \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocessed_slice_thrices\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument tensors in method wrapper_CUDA_cat)"
     ]
    }
   ],
   "source": [
    "ct_embedding_model = CTModel()\n",
    "x = torch.randn(1, 384, 512, 512)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "ct_embedding_model.to(device)\n",
    "x = x.to(device)\n",
    "\n",
    "out = ct_embedding_model(x)\n",
    "\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "722060c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c281cab3-a86a-428b-9ccb-307f515c182a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, random_split, RandomSampler, BatchSampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "import time\n",
    "import logging\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "class TripletModel:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        mode='HN',\n",
    "        seed=42,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        mode: (negative sampling) \"random\" or \"HN\"\n",
    "        estimator: \"Linear\" or \"net\"\n",
    "        \"\"\"\n",
    "\n",
    "        # dim latent space\n",
    "        #self.factor = factor \n",
    "\n",
    "        #VSE++\n",
    "        #self.batch_size = 30  \n",
    "\n",
    "        #ResNet18\n",
    "        self.batch_size = 8\n",
    "\n",
    "        #VSE++\n",
    "        #self.epochs = 30\n",
    "\n",
    "        # ResNet18\n",
    "        self.epochs = 1\n",
    "        \n",
    "        self.model = model\n",
    "        \n",
    "        self.device = None\n",
    "        \n",
    "        if not torch.cuda.is_available():\n",
    "            self.device = torch.device(\"cuda\")\n",
    "            self.model.to(self.device)\n",
    "        \n",
    "        #VSE++\n",
    "        self.optimizer = torch.optim.Adam(params=self.model.parameters(), lr=0.0002)\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=15, gamma=0.1)\n",
    "\n",
    "        # FF net or LINEAR\n",
    "        #self.estimator = estimator\n",
    "        \n",
    "        # Save checkpoints to this path\n",
    "        self.checkpoint_path = TRIPLET_MODELS_PATH\n",
    "        \n",
    "        # Negative sampling: 'HN' or 'random'\n",
    "        self.mode = mode\n",
    "        \n",
    "        # Triplet loss' alpha\n",
    "        self.alpha = 0.2\n",
    "        \n",
    "        self.seed = seed\n",
    "    \n",
    "    def get_triplets(self, ids_list):\n",
    "        # construct positive pairs and negative-compatibles from the data points in the mini-batch\n",
    "        #print('ids_list:', ids_list)\n",
    "        positive_pairs, negative_compatibles = self.dataset.get_batch_positive_negative_pairs(ids_list)\n",
    "        triplets = list()\n",
    "\n",
    "        if len(positive_pairs) == 0 or len(negative_compatibles) == 0:\n",
    "            logging.warning(f\"Cannot build triplets from current batch! {len(positive_pairs)} positives and {len(negative_compatibles)} negative pair candidates found.\")\n",
    "            return None #skip batch\n",
    "\n",
    "        if self.mode == \"random\":\n",
    "            negative_pairs = [[a, random.choice(negative_compatibles[neg_id])] for (a, neg_id) in negative_compatibles]\n",
    "            # a_p and a_n should always be the same value, so it doesn't matter which one we choose to build the triplet\n",
    "            triplets = torch.tensor([[a_p, p, n] for ([a_p, p], [a_n, n]) in zip(positive_pairs, negative_pairs)])\n",
    "\n",
    "        elif self.mode == \"HN\":\n",
    "            anchors_list = [a for (a, _) in positive_pairs]\n",
    "            #embeddings = self.dataset.get_embeddings(ids_list)\n",
    "            anchors_embs = self.dataset.get_embeddings(anchors_list)\n",
    "            negative_pairs = list()\n",
    "            for anchor_id, anchor_emb in zip(anchors_list, anchors_embs):\n",
    "                negatives_compatibles_ids = negative_compatibles[anchor_id]\n",
    "                n_compatibles = len(negatives_compatibles_ids)\n",
    "                negatives_compatibles_embs = self.dataset.get_embeddings(negatives_compatibles_ids)\n",
    "                negatives_compatibles_embs = negatives_compatibles_embs.squeeze(1)\n",
    "                anchor_emb_repeat = anchor_emb.repeat(n_compatibles, 1)\n",
    "                #anchor_emb_repeat = torch.fill(torch.empty(n_compatibles), anchor_emb)\n",
    "                a_n_pairwise_similarities = torch.nn.functional.cosine_similarity(anchor_emb_repeat, negatives_compatibles_embs)\n",
    "                most_similar_id = torch.argmax(a_n_pairwise_similarities)\n",
    "                negative_pairs.append([anchor_id, negatives_compatibles_ids[most_similar_id]])\n",
    "                #print('\\nSTART ANCHOR\\n')\n",
    "                #print('anchors_list', anchors_list)\n",
    "                #print('anchor_id', anchor_id)\n",
    "                #print('negatives_compatibles_ids', negatives_compatibles_ids)\n",
    "                #print('anchor_emb_repeat', anchor_emb_repeat.shape, anchor_emb_repeat)\n",
    "                #print('negatives_compatibles_embs', negatives_compatibles_embs.shape, negatives_compatibles_embs)\n",
    "                #print('a_n_pairwise_similarities', a_n_pairwise_similarities)\n",
    "                #print('most_similar_id', most_similar_id)\n",
    "                #print('\\nEND ANCHOR\\n')\n",
    "            # a_p and a_n should always be the same value, so it doesn't matter which one we choose to build the triplet\n",
    "            triplets = torch.tensor([[a_p, p, n] for ([a_p, p], [a_n, n]) in zip(positive_pairs, negative_pairs)])\n",
    "        if self.device:\n",
    "            triplets = triplets.to(self.device)\n",
    "        return triplets\n",
    "        \n",
    "        \n",
    "    def training(self, dataset, train_frac=0.6, batch_size=8, epochs=1):\n",
    "        \"\"\"\n",
    "        Training process\n",
    "        \"\"\"\n",
    "        \n",
    "        assert train_frac > 0.0 and 1.0 >= train_frac\n",
    "        \n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        \n",
    "        # Define the sizes of train, validation, and test sets\n",
    "        train_size = int(train_frac * len(self.dataset))  # percentage of the data for training\n",
    "        val_size = int((1.0 - train_frac)/1 * len(self.dataset)) # half of the remaining for validation\n",
    "        test_size = len(self.dataset) - train_size - val_size  # the rest of the remaining for testing\n",
    "\n",
    "        '''\n",
    "        If necessary, adjust the size of the training set when the sum of \n",
    "        the sizes of the three sets differs from the total dataset size.\n",
    "        '''\n",
    "        size_diff = len(self.dataset) - train_size - val_size - test_size\n",
    "        train_size += size_diff\n",
    "\n",
    "        # Use random_split to split the dataset\n",
    "        self.train_dataset, self.val_dataset, self.test_dataset = random_split(dataset=self.dataset, lengths=[train_size, val_size, test_size], generator=torch.Generator().manual_seed(self.seed))\n",
    "        \n",
    "        # Create a RandomSampler with a predetermined seed and no replacement\n",
    "        #self.training_sampler = RandomSampler(self.train_dataset, replacement=False, generator=torch.Generator().manual_seed(seed))\n",
    "        self.validation_sampler = RandomSampler(self.val_dataset, replacement=False, generator=torch.Generator().manual_seed(self.seed))\n",
    "        self.test_sampler = RandomSampler(self.test_dataset, replacement=False, generator=torch.Generator().manual_seed(self.seed))\n",
    "\n",
    "        #self.train_batch_sampler = BatchSampler(sampler=self.training_sampler, batch_size=self.batch_size, drop_last=True)\n",
    "        self.val_batch_sampler = BatchSampler(sampler=self.validation_sampler, batch_size=len(self.val_dataset), drop_last=True)\n",
    "        self.test_batch_sampler = BatchSampler(sampler=self.test_sampler, batch_size=len(self.test_dataset), drop_last=True)\n",
    "        \n",
    "        # self.dataset.shape := (number of data points, number of classes)\n",
    "        #self.n_classes = self.dataset.shape[1]\n",
    "\n",
    "        self.recalls = [] # recall 1, 10, 25\n",
    "        self.loss_per_epoch = []\n",
    "        self.step_losses = []\n",
    "        self.val_loss_per_epoch = []\n",
    "        \n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        self.tb_writer = SummaryWriter(os.path.join(TRIPLET_MODELS_PATH, \"runs\", \"ct_retrieval_trainer_{}\".format(timestamp)))\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            logging.info(f\"\\n--------------------------------\\nEpoch {epoch+1} of {self.epochs}\\n--------------------------------\\n\")\n",
    "\n",
    "            batch_counter = 0 # counter of processed batches across training\n",
    "            \n",
    "            epoch_losses = list()\n",
    "            \n",
    "            # change the random seed and reconstruct training_sampler and train_batch_sampler at each epoch\n",
    "            # this way we can assemble never-before-seen triplets to train on.\n",
    "            self.training_sampler = RandomSampler(self.train_dataset, replacement=False, generator=torch.Generator().manual_seed(self.seed + epoch))\n",
    "            self.train_batch_sampler = BatchSampler(sampler=self.training_sampler, batch_size=self.batch_size, drop_last=True)\n",
    "\n",
    "            # recalculate dataset embeddings at this epoch\n",
    "            dataset_embeddings = self.dataset.get_updated_embeddings(self.model, self.device)\n",
    "            \n",
    "            for batch_index, training_batch in enumerate(self.train_batch_sampler):\n",
    "                batch_counter += 1\n",
    "                logging.info(f\"=> Batch {batch_counter}:\")\n",
    "\n",
    "                # load batch data\n",
    "                dataset_batch_ids, batch_embeddings, batch_labels = self.train_dataset[training_batch]\n",
    "\n",
    "                batch_id_map = dict((i, j) for (i, j) in zip(dataset_batch_ids.tolist(), training_batch))\n",
    "\n",
    "                # dataset_batch_ids != training_batch bacause dataset_batch_ids corresponds to the original\n",
    "                # unsplitted dataset while training_batch maps to the training dataset\n",
    "\n",
    "                if self.device is not None:\n",
    "                    batch_embeddings = batch_embeddings.to(self.device)\n",
    "                    \n",
    "                batch_triplets = self.get_triplets(dataset_batch_ids.tolist())\n",
    "                batch_triplets = [[batch_id_map[a], batch_id_map[p], batch_id_map[n]] for [a, p, n] in batch_triplets.tolist()]\n",
    "                batch_triplets = torch.tensor(batch_triplets)\n",
    "                #print('training_batch:', training_batch)\n",
    "                #print('dataset_batch_ids:', dataset_batch_ids)\n",
    "                #print('batch_triplets:', batch_triplets)\n",
    "                \n",
    "                if batch_triplets is None:\n",
    "                    # skip empty batch\n",
    "                    continue\n",
    "                \n",
    "                # perform forward pass over all samples in the batch\n",
    "                self.model.train(True)\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(batch_embeddings)\n",
    "\n",
    "                # map the dataset IDs referenced in the triplets to the indices in the batched model output\n",
    "                #print('batch_ids:', mini_batch)\n",
    "                #print('triplets before mapping:\\n', triplets)\n",
    "                #triplets = self._map_triplets(mini_batch, triplets)\n",
    "                #print('triplets after mapping:\\n', triplets)\n",
    "                \n",
    "                # calculate loss and gradients\n",
    "                # IMPORTANT: the loss is calculated for each triplet (3-tuples of samples), therefore,\n",
    "                # if there is a sample which is not part of a triplet, then it will not be needed in\n",
    "                # calculation.\n",
    "                batch_loss = self.loss(outputs, training_batch, batch_triplets, alpha=self.alpha)\n",
    "                batch_loss.requires_grad = True\n",
    "                batch_loss.backward()\n",
    "\n",
    "                # update model weights\n",
    "                self.optimizer.step()\n",
    "\n",
    "                self.tb_writer.add_scalar('Steps: Loss/train', batch_loss, global_step=batch_counter)\n",
    "                \n",
    "                epoch_losses.append(batch_loss)\n",
    "                self.step_losses.append(batch_loss)\n",
    "                logging.info(f\"Batch {batch_counter} loss: {round(batch_loss.item(), 4)}\")\n",
    "            \n",
    "            # finished iterating the batches\n",
    "            epoch_mean_loss = (sum(epoch_losses) / len(epoch_losses)).item()\n",
    "            self.tb_writer.add_scalar('Epochs: Loss/train', epoch_mean_loss, global_step=epoch+1)\n",
    "            self.loss_per_epoch.append(epoch_mean_loss)\n",
    "\n",
    "            logging.info(f'Training loss: {round(epoch_mean_loss, 4)}')\n",
    "            \n",
    "            \n",
    "            # evaluate on validation set\n",
    "            logging.info(\"Evaluating...\")\n",
    "            self.model.eval()\n",
    "            val_epoch_losses = list()\n",
    "            with torch.no_grad():\n",
    "                for val_batch_index, val_batch in enumerate(self.val_batch_sampler):\n",
    "\n",
    "                    val_batch_ids, val_batch_embeddings, val_batch_labels = self.val_dataset[val_batch]\n",
    "\n",
    "                    val_batch_id_map = dict((i, j) for (i, j) in zip(val_batch_ids.tolist(), val_batch))\n",
    "                    \n",
    "\n",
    "                    if self.device is not None:\n",
    "                        val_batch_embeddings = val_batch_embeddings.to(self.device)\n",
    "                        \n",
    "                    val_batch_triplets = self.get_triplets(val_batch_ids.tolist())\n",
    "                    val_batch_triplets = [[val_batch_id_map[a], val_batch_id_map[p], val_batch_id_map[n]] for [a, p, n] in val_batch_triplets.tolist()]\n",
    "                    val_batch_triplets = torch.tensor(val_batch_triplets)\n",
    "                    \n",
    "                    if val_batch_triplets is None:\n",
    "                        # skip validation if there was no triplets to evaluate\n",
    "                        continue\n",
    "\n",
    "                    # map the dataset IDs referenced in the triplets to the indices in the batched model output\n",
    "\n",
    "                    #val_triplets = self._map_triplets(val_batch, val_triplets)\n",
    "\n",
    "                    \n",
    "\n",
    "                    # perform evaluation on validation data\n",
    "                    val_outputs = self.model(val_batch_embeddings)\n",
    "\n",
    "                    # calculate validation loss\n",
    "                    val_batch_loss = self.loss(val_outputs, val_batch, val_batch_triplets, alpha=self.alpha)\n",
    "                    val_epoch_losses.append(val_batch_loss)\n",
    "                    logging.info(f\"Validation batch {val_batch_index+1} loss: {round(val_batch_loss.item(), 4)}\")\n",
    "                \n",
    "                # finished iterating the batches\n",
    "                val_mean_loss = (sum(val_epoch_losses) / len(val_epoch_losses)).item()\n",
    "                self.tb_writer.add_scalars(\n",
    "                    'Training vs. Validation Loss',\n",
    "                    {\n",
    "                        'Training': epoch_mean_loss, \n",
    "                        'Validation': val_mean_loss,\n",
    "                    }, \n",
    "                    epoch + 1\n",
    "                )\n",
    "                self.val_loss_per_epoch.append(val_mean_loss)\n",
    "\n",
    "                logging.info(f'Validation loss: {round(val_mean_loss, 4)}')\n",
    "            \n",
    "                # end batch iteration\n",
    "            logging.info(\"Done evaluating.\")\n",
    "            if(self.val_loss_per_epoch[-1] == min(self.val_loss_per_epoch)):\n",
    "                logging.info(\"Best validation loss achieved! Saving checkpoint...\")\n",
    "                self.save(\n",
    "                    epoch,\n",
    "                    self.loss_per_epoch, \n",
    "                    self.step_losses, \n",
    "                    self.val_loss_per_epoch, \n",
    "                    self.recalls,\n",
    "                    time.time()-start_time,\n",
    "                    os.path.join(TRIPLET_CHECKPOINTS_PATH, f'ct_retrieval_trainer_{timestamp}.pth'),\n",
    "                )\n",
    "                logging.info(\"Checkpoint saved.\")\n",
    "\n",
    "            # update optimizer scheduler ater completing an epoch\n",
    "            self.scheduler.step()\n",
    "            # end epoch iteration\n",
    "        #print('train_dataset:', self.train_dataset[0:len(self.train_dataset)][0])\n",
    "        #print('val_dataset:', self.val_dataset[0:len(self.val_dataset)][0])\n",
    "\n",
    "    def _map_triplets(self, batch_ids, triplets):\n",
    "        a_tensor = triplets.select(1, 0)\n",
    "        p_tensor = triplets.select(1, 1)\n",
    "        n_tensor = triplets.select(1, 2)\n",
    "        a_items = [e.item() for e in a_tensor]\n",
    "        p_items = [e.item() for e in p_tensor]\n",
    "        n_items = [e.item() for e in n_tensor]\n",
    "        a_index = list()\n",
    "        p_index = list()\n",
    "        n_index = list()\n",
    "        for (a, p, n) in zip(a_items, p_items, n_items):\n",
    "            a_index.append(batch_ids.index(a))\n",
    "            p_index.append(batch_ids.index(p))\n",
    "            n_index.append(batch_ids.index(n))\n",
    "        mapped_triplets = torch.tensor([a_index, p_index, n_index])\n",
    "        mapped_triplets = mapped_triplets.transpose(0, 1)\n",
    "        return mapped_triplets\n",
    "\n",
    "    def _max(self, a, b):\n",
    "        if a > b:\n",
    "            return a\n",
    "        return b\n",
    "    \n",
    "    def loss(self, embeddings, embeddings_ids, triplets_ids, alpha=0.2):\n",
    "        \n",
    "        \"\"\"\n",
    "        for computing the triplet loss [alpha + negative_similarity - positive_similarity]_{+}\n",
    "        \"\"\"\n",
    "        anchors_ids = torch.select(triplets_ids, 1, 0).tolist()\n",
    "        positives_ids = torch.select(triplets_ids, 1, 1).tolist()\n",
    "        negatives_ids = torch.select(triplets_ids, 1, 2).tolist()\n",
    "\n",
    "        ids_map = dict((j, i) for i, j in enumerate(embeddings_ids))\n",
    "        \n",
    "        anchors = embeddings[[ids_map[a] for a in anchors_ids]]\n",
    "        positives = embeddings[[ids_map[p] for p in positives_ids]]\n",
    "        negatives = embeddings[[ids_map[n] for n in negatives_ids]]\n",
    "        \n",
    "        cos = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "        \n",
    "        positive_similarities = cos(anchors, positives)\n",
    "        negative_similarities = cos(anchors, negatives)\n",
    "        \n",
    "        unrectified_batch_loss = alpha + negative_similarities - positive_similarities\n",
    "        rectified_batch_loss = unrectified_batch_loss.detach().map_(torch.zeros(len(unrectified_batch_loss)), self._max)\n",
    "        #rectified_batch_loss = torch.func.vmap(torch.max)(unrectified_batch_loss, torch.tensor(0))\n",
    "        batch_loss = torch.mean(rectified_batch_loss)\n",
    "        if self.device:\n",
    "            batch_loss = batch_loss.to(device)\n",
    "        return batch_loss\n",
    "    \n",
    "    def save(self, epoch, epoch_loss, step_loss, val_loss, recalls, running_time, directory):\n",
    "        \"\"\"\n",
    "        For saving logs of the experiment\n",
    "        \"\"\"\n",
    "        \n",
    "        torch.save(\n",
    "            {\n",
    "                'model_state_dict': self.model.state_dict(),\n",
    "                'optimizer_state_dict': self.scheduler.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'epoch_loss': epoch_loss,\n",
    "                'step_loss': step_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'recalls': recalls,\n",
    "                'running_time': running_time,\n",
    "            },\n",
    "            directory\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0cf07a1b-3d4a-4b6c-ba0b-e176912bd55f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    \\n        def predict(self,query,split):\\n        \"\"\"\\n        the split defines where we are going to search\\n        \"\"\"\\n        if type(query) is not list:\\n            query= [query]\\n\\n        i_input = self.visual_encoder(tf.nn.embedding_lookup(visual_matrix, query))\\n        if split==\"val\":\\n                c_eval= self.visual_encoder(visual_matrix[self.val_ids])\\n        if split==\"test\":\\n                c_eval= self.visual_encoder(visual_matrix[self.test_ids])\\n        if split==\"train\":\\n                c_eval=self.visual_encoder(visual_matrix[self.train_ids])\\n\\n        rating= tf.matmul(i_input, tf.transpose(c_eval) )\\n        return np.reshape(rating, [-1])\\n\\n    def metric(self, split, k=10):\\n        \"\"\"\\n        to compute metrics\\n        \"\"\"\\n        if split == \"val\":\\n            ids, gt = self.val_ids, self.labels_val\\n        if split == \"test\":\\n            ids, gt = self.test_ids, self.labels_test\\n\\n        metric=0\\n\\n        for i in range(len(ids)): #querys\\n            top= np.argsort(self.predict(ids[i],split))[::-1][1:k+1] #first one must be the query\\n            #get labels\\n            prediction= [gt[j] for j in top]\\n            metric+=recall(gt[i], prediction)\\n        return metric/len(ids) #average\\n\\n    def inference(self):\\n        \"\"\"\\n        testing phase\\n        \"\"\"\\n        results=(self.metric(\"test\",k=1), self.metric(\"test\",k=10), self.metric(\"test\",k=25))\\n        print(results)\\n        np.save(self.DIS_MODEL_FILE+\"test_metrics.npy\", np.array([results], dtype=object) )\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''    \n",
    "        def predict(self,query,split):\n",
    "        \"\"\"\n",
    "        the split defines where we are going to search\n",
    "        \"\"\"\n",
    "        if type(query) is not list:\n",
    "            query= [query]\n",
    "\n",
    "        i_input = self.visual_encoder(tf.nn.embedding_lookup(visual_matrix, query))\n",
    "        if split==\"val\":\n",
    "                c_eval= self.visual_encoder(visual_matrix[self.val_ids])\n",
    "        if split==\"test\":\n",
    "                c_eval= self.visual_encoder(visual_matrix[self.test_ids])\n",
    "        if split==\"train\":\n",
    "                c_eval=self.visual_encoder(visual_matrix[self.train_ids])\n",
    "\n",
    "        rating= tf.matmul(i_input, tf.transpose(c_eval) )\n",
    "        return np.reshape(rating, [-1])\n",
    "\n",
    "    def metric(self, split, k=10):\n",
    "        \"\"\"\n",
    "        to compute metrics\n",
    "        \"\"\"\n",
    "        if split == \"val\":\n",
    "            ids, gt = self.val_ids, self.labels_val\n",
    "        if split == \"test\":\n",
    "            ids, gt = self.test_ids, self.labels_test\n",
    "\n",
    "        metric=0\n",
    "\n",
    "        for i in range(len(ids)): #querys\n",
    "            top= np.argsort(self.predict(ids[i],split))[::-1][1:k+1] #first one must be the query\n",
    "            #get labels\n",
    "            prediction= [gt[j] for j in top]\n",
    "            metric+=recall(gt[i], prediction)\n",
    "        return metric/len(ids) #average\n",
    "\n",
    "    def inference(self):\n",
    "        \"\"\"\n",
    "        testing phase\n",
    "        \"\"\"\n",
    "        results=(self.metric(\"test\",k=1), self.metric(\"test\",k=10), self.metric(\"test\",k=25))\n",
    "        print(results)\n",
    "        np.save(self.DIS_MODEL_FILE+\"test_metrics.npy\", np.array([results], dtype=object) )\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9a6feaeb-6e3d-4a71-a31c-48658b26116d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Data point with dataset_id=7 will be discarded because its corresponding visual embedding file does not exist: C:\\Users\\User\\Documents\\Proyecto Proximity\\DR70\\visual_embeddings\\resnet18\\reshaped_averaged\\1.3.12.2.1107.5.1.4.83504.30000020021012090131800045301.npy\n",
      "INFO:root:\n",
      "--------------------------------\n",
      "Epoch 1 of 10\n",
      "--------------------------------\n",
      "\n",
      "INFO:root:=> Batch 1:\n",
      "INFO:root:Generating triplets for batch [67, 47, 18, 55, 39, 12, 45, 28]\n",
      "INFO:root:Could not construct a positive pair for anchor ID 45 in this batch. Skipping...\n",
      "INFO:root:Batch 1 loss: 0.2585\n",
      "INFO:root:=> Batch 2:\n",
      "INFO:root:Generating triplets for batch [50, 48, 53, 62, 15, 44, 19, 11]\n",
      "INFO:root:Batch 2 loss: 0.2311\n",
      "INFO:root:=> Batch 3:\n",
      "INFO:root:Generating triplets for batch [25, 34, 56, 6, 41, 31, 21, 61]\n",
      "INFO:root:Could not construct a positive pair for anchor ID 25 in this batch. Skipping...\n",
      "INFO:root:Could not construct a positive pair for anchor ID 56 in this batch. Skipping...\n",
      "INFO:root:Batch 3 loss: 0.2167\n",
      "INFO:root:=> Batch 4:\n",
      "INFO:root:Generating triplets for batch [43, 3, 42, 35, 68, 32, 30, 23]\n",
      "INFO:root:Could not construct a positive pair for anchor ID 35 in this batch. Skipping...\n",
      "INFO:root:Batch 4 loss: 0.2619\n",
      "INFO:root:=> Batch 5:\n",
      "INFO:root:Generating triplets for batch [9, 4, 0, 60, 49, 57, 40, 8]\n",
      "INFO:root:Could not construct a positive pair for anchor ID 40 in this batch. Skipping...\n",
      "INFO:root:Batch 5 loss: 0.206\n",
      "INFO:root:=> Batch 6:\n",
      "INFO:root:Generating triplets for batch [26, 14, 17, 65, 63, 51, 16, 58]\n",
      "INFO:root:Could not construct a positive pair for anchor ID 65 in this batch. Skipping...\n",
      "INFO:root:Could not construct a positive pair for anchor ID 63 in this batch. Skipping...\n",
      "INFO:root:Could not construct a positive pair for anchor ID 58 in this batch. Skipping...\n",
      "INFO:root:Batch 6 loss: 0.2372\n",
      "INFO:root:Training loss: 0.2352\n",
      "INFO:root:Evaluating...\n",
      "INFO:root:Generating triplets for batch [29, 5, 22, 46, 66, 27, 10, 52, 7, 13, 37, 38, 20, 54, 59, 33, 2, 64, 1, 36]\n",
      "INFO:root:Could not construct a positive pair for anchor ID 1 in this batch. Skipping...\n",
      "INFO:root:Validation batch 1 loss: 0.2243\n",
      "INFO:root:Validation loss: 0.2243\n",
      "INFO:root:Done evaluating.\n",
      "INFO:root:Best validation loss achieved! Saving checkpoint...\n",
      "INFO:root:Checkpoint saved.\n",
      "INFO:root:\n",
      "--------------------------------\n",
      "Epoch 2 of 10\n",
      "--------------------------------\n",
      "\n",
      "INFO:root:=> Batch 1:\n",
      "INFO:root:Generating triplets for batch [60, 47, 42, 58, 44, 43, 65, 49]\n",
      "INFO:root:Could not construct a positive pair for anchor ID 58 in this batch. Skipping...\n",
      "INFO:root:Could not construct a positive pair for anchor ID 65 in this batch. Skipping...\n",
      "INFO:root:Batch 1 loss: 0.2324\n",
      "INFO:root:=> Batch 2:\n",
      "INFO:root:Generating triplets for batch [61, 14, 8, 16, 21, 35, 41, 40]\n",
      "INFO:root:Could not construct a positive pair for anchor ID 35 in this batch. Skipping...\n",
      "INFO:root:Could not construct a positive pair for anchor ID 40 in this batch. Skipping...\n",
      "INFO:root:Batch 2 loss: 0.2351\n",
      "INFO:root:=> Batch 3:\n",
      "INFO:root:Generating triplets for batch [25, 68, 39, 0, 32, 63, 11, 6]\n",
      "INFO:root:Could not construct a positive pair for anchor ID 25 in this batch. Skipping...\n",
      "INFO:root:Batch 3 loss: 0.242\n",
      "INFO:root:=> Batch 4:\n",
      "INFO:root:Generating triplets for batch [28, 55, 57, 30, 4, 19, 51, 53]\n",
      "INFO:root:Could not construct a positive pair for anchor ID 28 in this batch. Skipping...\n",
      "INFO:root:Batch 4 loss: 0.2376\n",
      "INFO:root:=> Batch 5:\n",
      "INFO:root:Generating triplets for batch [23, 18, 31, 26, 15, 45, 3, 17]\n",
      "INFO:root:Could not construct a positive pair for anchor ID 18 in this batch. Skipping...\n",
      "INFO:root:Could not construct a positive pair for anchor ID 45 in this batch. Skipping...\n",
      "INFO:root:Batch 5 loss: 0.2126\n",
      "INFO:root:=> Batch 6:\n",
      "INFO:root:Generating triplets for batch [48, 62, 67, 9, 12, 50, 56, 34]\n",
      "INFO:root:Batch 6 loss: 0.2\n",
      "INFO:root:Training loss: 0.2266\n",
      "INFO:root:Evaluating...\n",
      "INFO:root:Generating triplets for batch [33, 66, 52, 46, 59, 5, 2, 37, 7, 22, 29, 27, 36, 38, 10, 20, 64, 1, 13, 54]\n",
      "INFO:root:Could not construct a positive pair for anchor ID 1 in this batch. Skipping...\n",
      "INFO:root:Validation batch 1 loss: 0.2315\n",
      "INFO:root:Validation loss: 0.2315\n",
      "INFO:root:Done evaluating.\n",
      "INFO:root:\n",
      "--------------------------------\n",
      "Epoch 3 of 10\n",
      "--------------------------------\n",
      "\n",
      "INFO:root:=> Batch 1:\n",
      "INFO:root:Generating triplets for batch [60, 6, 42, 61, 9, 45, 49, 55]\n",
      "INFO:root:Could not construct a positive pair for anchor ID 61 in this batch. Skipping...\n",
      "INFO:root:Could not construct a positive pair for anchor ID 45 in this batch. Skipping...\n",
      "INFO:root:Batch 1 loss: 0.222\n",
      "INFO:root:=> Batch 2:\n",
      "INFO:root:Generating triplets for batch [25, 67, 23, 48, 18, 34, 40, 4]\n",
      "INFO:root:Could not construct a positive pair for anchor ID 25 in this batch. Skipping...\n",
      "INFO:root:Could not construct a positive pair for anchor ID 23 in this batch. Skipping...\n",
      "INFO:root:Could not construct a positive pair for anchor ID 40 in this batch. Skipping...\n",
      "INFO:root:Batch 2 loss: 0.2487\n",
      "INFO:root:=> Batch 3:\n",
      "INFO:root:Generating triplets for batch [15, 35, 63, 68, 50, 3, 53, 65]\n",
      "INFO:root:Could not construct a positive pair for anchor ID 68 in this batch. Skipping...\n",
      "INFO:root:Batch 3 loss: 0.2393\n",
      "INFO:root:=> Batch 4:\n",
      "INFO:root:Generating triplets for batch [56, 16, 14, 17, 51, 21, 19, 41]\n",
      "INFO:root:Could not construct a positive pair for anchor ID 56 in this batch. Skipping...\n",
      "INFO:root:Could not construct a positive pair for anchor ID 21 in this batch. Skipping...\n",
      "INFO:root:Batch 4 loss: 0.2228\n",
      "INFO:root:=> Batch 5:\n",
      "INFO:root:Generating triplets for batch [11, 30, 12, 28, 57, 26, 0, 62]\n",
      "INFO:root:Could not construct a positive pair for anchor ID 30 in this batch. Skipping...\n",
      "INFO:root:Could not construct a positive pair for anchor ID 28 in this batch. Skipping...\n",
      "INFO:root:Batch 5 loss: 0.2285\n",
      "INFO:root:=> Batch 6:\n",
      "INFO:root:Generating triplets for batch [8, 43, 47, 39, 58, 31, 32, 44]\n",
      "INFO:root:Batch 6 loss: 0.2375\n",
      "INFO:root:Training loss: 0.2331\n",
      "INFO:root:Evaluating...\n",
      "INFO:root:Generating triplets for batch [38, 5, 27, 36, 29, 7, 33, 22, 20, 52, 2, 13, 46, 37, 64, 54, 66, 1, 10, 59]\n",
      "INFO:root:Could not construct a positive pair for anchor ID 1 in this batch. Skipping...\n",
      "INFO:root:Validation batch 1 loss: 0.2316\n",
      "INFO:root:Validation loss: 0.2316\n",
      "INFO:root:Done evaluating.\n",
      "INFO:root:\n",
      "--------------------------------\n",
      "Epoch 4 of 10\n",
      "--------------------------------\n",
      "\n",
      "INFO:root:=> Batch 1:\n",
      "INFO:root:Generating triplets for batch [3, 18, 55, 39, 15, 31, 32, 43]\n",
      "INFO:root:Batch 1 loss: 0.2328\n",
      "INFO:root:=> Batch 2:\n",
      "INFO:root:Generating triplets for batch [61, 58, 25, 23, 34, 16, 50, 28]\n",
      "INFO:root:Could not construct a positive pair for anchor ID 25 in this batch. Skipping...\n",
      "INFO:root:Batch 2 loss: 0.2106\n",
      "INFO:root:=> Batch 3:\n",
      "INFO:root:Generating triplets for batch [63, 35, 21, 6, 44, 17, 62, 4]\n",
      "INFO:root:Could not construct a positive pair for anchor ID 35 in this batch. Skipping...\n",
      "INFO:root:Batch 3 loss: 0.2123\n",
      "INFO:root:=> Batch 4:\n",
      "INFO:root:Generating triplets for batch [49, 26, 60, 56, 42, 0, 30, 45]\n",
      "INFO:root:Could not construct a positive pair for anchor ID 56 in this batch. Skipping...\n",
      "INFO:root:Could not construct a positive pair for anchor ID 30 in this batch. Skipping...\n",
      "INFO:root:Could not construct a positive pair for anchor ID 45 in this batch. Skipping...\n",
      "INFO:root:Batch 4 loss: 0.1812\n",
      "INFO:root:=> Batch 5:\n",
      "INFO:root:Generating triplets for batch [12, 8, 41, 65, 68, 48, 47, 57]\n",
      "INFO:root:Could not construct a positive pair for anchor ID 65 in this batch. Skipping...\n",
      "INFO:root:Could not construct a positive pair for anchor ID 68 in this batch. Skipping...\n",
      "INFO:root:Could not construct a positive pair for anchor ID 47 in this batch. Skipping...\n",
      "INFO:root:Batch 5 loss: 0.2086\n",
      "INFO:root:=> Batch 6:\n",
      "INFO:root:Generating triplets for batch [51, 67, 53, 19, 14, 40, 9, 11]\n",
      "INFO:root:Could not construct a positive pair for anchor ID 67 in this batch. Skipping...\n",
      "INFO:root:Could not construct a positive pair for anchor ID 53 in this batch. Skipping...\n",
      "INFO:root:Could not construct a positive pair for anchor ID 40 in this batch. Skipping...\n",
      "INFO:root:Batch 6 loss: 0.268\n",
      "INFO:root:Training loss: 0.2189\n",
      "INFO:root:Evaluating...\n",
      "INFO:root:Generating triplets for batch [27, 2, 52, 38, 13, 37, 29, 33, 59, 46, 10, 5, 64, 36, 54, 1, 66, 20, 22, 7]\n",
      "INFO:root:Could not construct a positive pair for anchor ID 1 in this batch. Skipping...\n",
      "INFO:root:Validation batch 1 loss: 0.2281\n",
      "INFO:root:Validation loss: 0.2281\n",
      "INFO:root:Done evaluating.\n",
      "INFO:root:\n",
      "--------------------------------\n",
      "Epoch 5 of 10\n",
      "--------------------------------\n",
      "\n",
      "INFO:root:=> Batch 1:\n",
      "INFO:root:Generating triplets for batch [56, 8, 68, 39, 57, 18, 35, 43]\n",
      "INFO:root:Could not construct a positive pair for anchor ID 39 in this batch. Skipping...\n",
      "INFO:root:Could not construct a positive pair for anchor ID 35 in this batch. Skipping...\n",
      "INFO:root:Batch 1 loss: 0.2336\n",
      "INFO:root:=> Batch 2:\n",
      "INFO:root:Generating triplets for batch [61, 32, 3, 45, 42, 17, 14, 67]\n",
      "INFO:root:Could not construct a positive pair for anchor ID 61 in this batch. Skipping...\n",
      "INFO:root:Could not construct a positive pair for anchor ID 45 in this batch. Skipping...\n",
      "INFO:root:Batch 2 loss: 0.2142\n",
      "INFO:root:=> Batch 3:\n",
      "INFO:root:Generating triplets for batch [34, 55, 51, 31, 53, 23, 25, 44]\n",
      "INFO:root:Could not construct a positive pair for anchor ID 25 in this batch. Skipping...\n",
      "INFO:root:Batch 3 loss: 0.2049\n",
      "INFO:root:=> Batch 4:\n",
      "INFO:root:Generating triplets for batch [19, 15, 40, 63, 21, 50, 41, 9]\n",
      "INFO:root:Could not construct a positive pair for anchor ID 40 in this batch. Skipping...\n",
      "INFO:root:Batch 4 loss: 0.2516\n",
      "INFO:root:=> Batch 5:\n",
      "INFO:root:Generating triplets for batch [60, 16, 62, 6, 30, 58, 48, 47]\n",
      "INFO:root:Could not construct a positive pair for anchor ID 58 in this batch. Skipping...\n",
      "INFO:root:Batch 5 loss: 0.2357\n",
      "INFO:root:=> Batch 6:\n",
      "INFO:root:Generating triplets for batch [49, 0, 28, 65, 26, 11, 12, 4]\n",
      "INFO:root:Could not construct a positive pair for anchor ID 28 in this batch. Skipping...\n",
      "INFO:root:Could not construct a positive pair for anchor ID 65 in this batch. Skipping...\n",
      "INFO:root:Batch 6 loss: 0.2244\n",
      "INFO:root:Training loss: 0.2274\n",
      "INFO:root:Evaluating...\n",
      "INFO:root:Generating triplets for batch [1, 52, 5, 27, 38, 22, 7, 10, 66, 13, 59, 29, 37, 20, 2, 64, 46, 36, 54, 33]\n",
      "INFO:root:Could not construct a positive pair for anchor ID 1 in this batch. Skipping...\n",
      "INFO:root:Validation batch 1 loss: 0.2337\n",
      "INFO:root:Validation loss: 0.2337\n",
      "INFO:root:Done evaluating.\n",
      "INFO:root:\n",
      "--------------------------------\n",
      "Epoch 6 of 10\n",
      "--------------------------------\n",
      "\n",
      "INFO:root:=> Batch 1:\n",
      "INFO:root:Generating triplets for batch [55, 16, 42, 34, 68, 18, 65, 58]\n",
      "INFO:root:Could not construct a positive pair for anchor ID 65 in this batch. Skipping...\n",
      "INFO:root:Batch 1 loss: 0.2285\n",
      "INFO:root:=> Batch 2:\n",
      "INFO:root:Generating triplets for batch [3, 62, 12, 53, 41, 61, 26, 50]\n",
      "INFO:root:Batch 2 loss: 0.2197\n",
      "INFO:root:=> Batch 3:\n",
      "INFO:root:Generating triplets for batch [35, 56, 11, 28, 0, 39, 60, 40]\n",
      "INFO:root:Could not construct a positive pair for anchor ID 35 in this batch. Skipping...\n",
      "INFO:root:Could not construct a positive pair for anchor ID 39 in this batch. Skipping...\n",
      "INFO:root:Could not construct a positive pair for anchor ID 40 in this batch. Skipping...\n",
      "INFO:root:Batch 3 loss: 0.2461\n",
      "INFO:root:=> Batch 4:\n",
      "INFO:root:Generating triplets for batch [63, 67, 49, 4, 44, 57, 43, 6]\n",
      "INFO:root:Could not construct a positive pair for anchor ID 67 in this batch. Skipping...\n",
      "INFO:root:Batch 4 loss: 0.2092\n",
      "INFO:root:=> Batch 5:\n",
      "INFO:root:Generating triplets for batch [25, 30, 17, 23, 8, 45, 51, 19]\n",
      "INFO:root:Batch 5 loss: 0.2485\n",
      "INFO:root:=> Batch 6:\n",
      "INFO:root:Generating triplets for batch [15, 14, 31, 48, 21, 32, 47, 9]\n",
      "INFO:root:Could not construct a positive pair for anchor ID 32 in this batch. Skipping...\n",
      "INFO:root:Batch 6 loss: 0.2334\n",
      "INFO:root:Training loss: 0.2309\n",
      "INFO:root:Evaluating...\n",
      "INFO:root:Generating triplets for batch [29, 64, 5, 1, 22, 37, 36, 54, 33, 66, 38, 2, 52, 13, 20, 10, 59, 7, 27, 46]\n",
      "INFO:root:Could not construct a positive pair for anchor ID 1 in this batch. Skipping...\n",
      "INFO:root:Validation batch 1 loss: 0.2222\n",
      "INFO:root:Validation loss: 0.2222\n",
      "INFO:root:Done evaluating.\n",
      "INFO:root:Best validation loss achieved! Saving checkpoint...\n",
      "INFO:root:Checkpoint saved.\n",
      "INFO:root:\n",
      "--------------------------------\n",
      "Epoch 7 of 10\n",
      "--------------------------------\n",
      "\n",
      "INFO:root:=> Batch 1:\n",
      "INFO:root:Generating triplets for batch [17, 0, 16, 60, 32, 49, 45, 6]\n",
      "INFO:root:Could not construct a positive pair for anchor ID 32 in this batch. Skipping...\n",
      "INFO:root:Could not construct a positive pair for anchor ID 45 in this batch. Skipping...\n",
      "INFO:root:Batch 1 loss: 0.1923\n",
      "INFO:root:=> Batch 2:\n",
      "INFO:root:Generating triplets for batch [57, 26, 68, 39, 55, 51, 12, 21]\n",
      "INFO:root:Could not construct a positive pair for anchor ID 68 in this batch. Skipping...\n",
      "INFO:root:Batch 2 loss: 0.2368\n",
      "INFO:root:=> Batch 3:\n",
      "INFO:root:Generating triplets for batch [34, 35, 15, 14, 53, 25, 41, 30]\n",
      "INFO:root:Batch 3 loss: 0.2262\n",
      "INFO:root:=> Batch 4:\n",
      "INFO:root:Generating triplets for batch [61, 4, 67, 56, 63, 40, 3, 44]\n",
      "INFO:root:Could not construct a positive pair for anchor ID 40 in this batch. Skipping...\n",
      "INFO:root:Batch 4 loss: 0.2253\n",
      "INFO:root:=> Batch 5:\n",
      "INFO:root:Generating triplets for batch [58, 8, 31, 18, 9, 47, 62, 28]\n",
      "INFO:root:Batch 5 loss: 0.2339\n",
      "INFO:root:=> Batch 6:\n",
      "INFO:root:Generating triplets for batch [11, 23, 43, 65, 42, 48, 19, 50]\n",
      "INFO:root:Could not construct a positive pair for anchor ID 23 in this batch. Skipping...\n",
      "INFO:root:Could not construct a positive pair for anchor ID 65 in this batch. Skipping...\n",
      "INFO:root:Batch 6 loss: 0.2215\n",
      "INFO:root:Training loss: 0.2227\n",
      "INFO:root:Evaluating...\n",
      "INFO:root:Generating triplets for batch [10, 29, 54, 37, 46, 5, 66, 33, 36, 38, 27, 7, 59, 1, 13, 20, 22, 64, 52, 2]\n",
      "INFO:root:Could not construct a positive pair for anchor ID 1 in this batch. Skipping...\n",
      "INFO:root:Validation batch 1 loss: 0.235\n",
      "INFO:root:Validation loss: 0.235\n",
      "INFO:root:Done evaluating.\n",
      "INFO:root:\n",
      "--------------------------------\n",
      "Epoch 8 of 10\n",
      "--------------------------------\n",
      "\n",
      "INFO:root:=> Batch 1:\n",
      "INFO:root:Generating triplets for batch [30, 31, 40, 12, 35, 44, 39, 14]\n",
      "INFO:root:Could not construct a positive pair for anchor ID 40 in this batch. Skipping...\n",
      "INFO:root:Could not construct a positive pair for anchor ID 35 in this batch. Skipping...\n",
      "INFO:root:Batch 1 loss: 0.2658\n",
      "INFO:root:=> Batch 2:\n",
      "INFO:root:Generating triplets for batch [41, 60, 67, 57, 61, 21, 49, 19]\n",
      "INFO:root:Could not construct a positive pair for anchor ID 67 in this batch. Skipping...\n",
      "INFO:root:Batch 2 loss: 0.2415\n",
      "INFO:root:=> Batch 3:\n",
      "INFO:root:Generating triplets for batch [47, 34, 58, 65, 45, 32, 48, 55]\n",
      "INFO:root:Could not construct a positive pair for anchor ID 47 in this batch. Skipping...\n",
      "INFO:root:Batch 3 loss: 0.2223\n",
      "INFO:root:=> Batch 4:\n",
      "INFO:root:Generating triplets for batch [6, 23, 25, 18, 17, 16, 26, 42]\n",
      "INFO:root:Could not construct a positive pair for anchor ID 23 in this batch. Skipping...\n",
      "INFO:root:Could not construct a positive pair for anchor ID 25 in this batch. Skipping...\n",
      "INFO:root:Could not construct a positive pair for anchor ID 18 in this batch. Skipping...\n",
      "INFO:root:Batch 4 loss: 0.2345\n",
      "INFO:root:=> Batch 5:\n",
      "INFO:root:Generating triplets for batch [4, 51, 9, 68, 8, 56, 50, 11]\n",
      "INFO:root:Batch 5 loss: 0.2413\n",
      "INFO:root:=> Batch 6:\n",
      "INFO:root:Generating triplets for batch [53, 28, 0, 63, 62, 15, 3, 43]\n",
      "INFO:root:Could not construct a positive pair for anchor ID 28 in this batch. Skipping...\n",
      "INFO:root:Batch 6 loss: 0.2206\n",
      "INFO:root:Training loss: 0.2377\n",
      "INFO:root:Evaluating...\n",
      "INFO:root:Generating triplets for batch [38, 52, 27, 10, 36, 37, 59, 13, 64, 1, 5, 29, 33, 20, 46, 2, 54, 22, 7, 66]\n",
      "INFO:root:Could not construct a positive pair for anchor ID 1 in this batch. Skipping...\n",
      "INFO:root:Validation batch 1 loss: 0.2277\n",
      "INFO:root:Validation loss: 0.2277\n",
      "INFO:root:Done evaluating.\n",
      "INFO:root:\n",
      "--------------------------------\n",
      "Epoch 9 of 10\n",
      "--------------------------------\n",
      "\n",
      "INFO:root:=> Batch 1:\n",
      "INFO:root:Generating triplets for batch [47, 31, 44, 60, 41, 11, 45, 0]\n",
      "INFO:root:Could not construct a positive pair for anchor ID 45 in this batch. Skipping...\n",
      "INFO:root:Batch 1 loss: 0.2328\n",
      "INFO:root:=> Batch 2:\n",
      "INFO:root:Generating triplets for batch [32, 67, 50, 8, 58, 9, 34, 26]\n",
      "INFO:root:Batch 2 loss: 0.2394\n",
      "INFO:root:=> Batch 3:\n",
      "INFO:root:Generating triplets for batch [19, 25, 35, 39, 55, 12, 15, 42]\n",
      "INFO:root:Could not construct a positive pair for anchor ID 39 in this batch. Skipping...\n",
      "INFO:root:Batch 3 loss: 0.2209\n",
      "INFO:root:=> Batch 4:\n",
      "INFO:root:Generating triplets for batch [21, 14, 53, 61, 4, 48, 16, 40]\n",
      "INFO:root:Could not construct a positive pair for anchor ID 40 in this batch. Skipping...\n",
      "INFO:root:Batch 4 loss: 0.244\n",
      "INFO:root:=> Batch 5:\n",
      "INFO:root:Generating triplets for batch [63, 68, 65, 43, 3, 62, 18, 56]\n",
      "INFO:root:Could not construct a positive pair for anchor ID 63 in this batch. Skipping...\n",
      "INFO:root:Could not construct a positive pair for anchor ID 65 in this batch. Skipping...\n",
      "INFO:root:Batch 5 loss: 0.1946\n",
      "INFO:root:=> Batch 6:\n",
      "INFO:root:Generating triplets for batch [30, 57, 23, 6, 17, 28, 49, 51]\n",
      "INFO:root:Could not construct a positive pair for anchor ID 28 in this batch. Skipping...\n",
      "INFO:root:Batch 6 loss: 0.2254\n",
      "INFO:root:Training loss: 0.2262\n",
      "INFO:root:Evaluating...\n",
      "INFO:root:Generating triplets for batch [66, 38, 13, 46, 2, 59, 54, 36, 52, 29, 22, 10, 5, 37, 27, 1, 64, 33, 20, 7]\n",
      "INFO:root:Could not construct a positive pair for anchor ID 1 in this batch. Skipping...\n",
      "INFO:root:Validation batch 1 loss: 0.2376\n",
      "INFO:root:Validation loss: 0.2376\n",
      "INFO:root:Done evaluating.\n",
      "INFO:root:\n",
      "--------------------------------\n",
      "Epoch 10 of 10\n",
      "--------------------------------\n",
      "\n",
      "INFO:root:=> Batch 1:\n",
      "INFO:root:Generating triplets for batch [45, 9, 34, 25, 31, 30, 12, 44]\n",
      "INFO:root:Batch 1 loss: 0.233\n",
      "INFO:root:=> Batch 2:\n",
      "INFO:root:Generating triplets for batch [23, 15, 48, 51, 35, 49, 42, 40]\n",
      "INFO:root:Could not construct a positive pair for anchor ID 23 in this batch. Skipping...\n",
      "INFO:root:Could not construct a positive pair for anchor ID 35 in this batch. Skipping...\n",
      "INFO:root:Could not construct a positive pair for anchor ID 40 in this batch. Skipping...\n",
      "INFO:root:Batch 2 loss: 0.2459\n",
      "INFO:root:=> Batch 3:\n",
      "INFO:root:Generating triplets for batch [39, 60, 17, 21, 16, 62, 53, 50]\n",
      "INFO:root:Batch 3 loss: 0.2246\n",
      "INFO:root:=> Batch 4:\n",
      "INFO:root:Generating triplets for batch [67, 6, 26, 47, 11, 8, 56, 3]\n",
      "INFO:root:Could not construct a positive pair for anchor ID 47 in this batch. Skipping...\n",
      "INFO:root:Batch 4 loss: 0.2295\n",
      "INFO:root:=> Batch 5:\n",
      "INFO:root:Generating triplets for batch [0, 28, 14, 43, 41, 57, 65, 18]\n",
      "INFO:root:Could not construct a positive pair for anchor ID 65 in this batch. Skipping...\n",
      "INFO:root:Batch 5 loss: 0.2204\n",
      "INFO:root:=> Batch 6:\n",
      "INFO:root:Generating triplets for batch [58, 19, 4, 32, 61, 68, 63, 55]\n",
      "INFO:root:Batch 6 loss: 0.2353\n",
      "INFO:root:Training loss: 0.2314\n",
      "INFO:root:Evaluating...\n",
      "INFO:root:Generating triplets for batch [22, 59, 10, 13, 7, 52, 36, 2, 20, 38, 1, 33, 37, 27, 64, 66, 46, 29, 5, 54]\n",
      "INFO:root:Could not construct a positive pair for anchor ID 1 in this batch. Skipping...\n",
      "INFO:root:Validation batch 1 loss: 0.2402\n",
      "INFO:root:Validation loss: 0.2402\n",
      "INFO:root:Done evaluating.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset: tensor([39, 32, 18, 61, 60, 12, 67, 55, 15, 62, 14,  3, 53, 31,  4, 19, 47, 44,\n",
      "        21, 25, 49, 41, 23, 26, 68, 58, 30, 42,  6, 48, 57, 16, 17,  8, 28, 43,\n",
      "        65, 11, 50,  0, 40, 63, 35,  9, 51, 56, 45, 34])\n",
      "val_dataset: tensor([10, 66, 29, 38, 52, 64,  7, 20, 37, 36, 27, 13, 33, 59,  5, 46,  1, 54,\n",
      "        22,  2])\n"
     ]
    }
   ],
   "source": [
    "ct_embedding_model = CTModel()\n",
    "triplet_model = TripletModel(model=ct_embedding_model, mode='HN', seed=42)\n",
    "\n",
    "# Create an instance of the dataset\n",
    "dr70_dataset = CustomDataset(CT_RESNET18_EMBEDDINGS_PATH, DR70_LABELS_PATH)\n",
    "\n",
    "triplet_model.training(dataset=dr70_dataset, train_frac=0.7, batch_size=8, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff008680-37dc-47d7-b93f-0213fbdb4be3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7830b3-ed39-4e23-afd5-f0d5b1a7fa45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
